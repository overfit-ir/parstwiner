{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "multi-task-training.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "OcqvEZ-r9frk"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNpOGER/1lEGQYIWfL63V60",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/overfit-ir/persian-twitter-ner/blob/feature%2Fadd-multi-task-learning/multi_task_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_MLN4DQ8cJc"
      },
      "source": [
        "### Get Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLJ97FgG9UvT",
        "outputId": "f331de65-d53f-4a74-def0-bf323071c0df"
      },
      "source": [
        "!rm run_tf_ner.py tasks.py utils_ner.py"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'run_tf_ner.py': No such file or directory\n",
            "rm: cannot remove 'tasks.py': No such file or directory\n",
            "rm: cannot remove 'utils_ner.py': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khyezUc9JFlE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "098e1fc2-4c48-4774-c0c7-d37ee6e239d1"
      },
      "source": [
        "!wget -q --show-progress --no-cache https://raw.githubusercontent.com/overfit-ir/persian-twitter-ner/feature/add-multi-task-learning/run_tf_ner.py\n",
        "!wget -q --show-progress --no-cache https://raw.githubusercontent.com/overfit-ir/persian-twitter-ner/feature/add-multi-task-learning/tasks.py\n",
        "!wget -q --show-progress --no-cache https://raw.githubusercontent.com/overfit-ir/persian-twitter-ner/feature/add-multi-task-learning/utils_ner.py"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "run_tf_ner.py       100%[===================>]  15.16K  --.-KB/s    in 0.001s  \n",
            "tasks.py            100%[===================>]   5.38K  --.-KB/s    in 0s      \n",
            "utils_ner.py        100%[===================>]  34.07K  --.-KB/s    in 0.003s  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7sVoYZR6okv",
        "outputId": "76b8bafa-47be-4c45-a311-5106de90db17"
      },
      "source": [
        "!wget -q --show-progress https://raw.githubusercontent.com/overfit-ir/persian-twitter-ner/master/twitter_data/persian-ner-twitter-data/train.txt\n",
        "!wget -q --show-progress https://raw.githubusercontent.com/overfit-ir/persian-twitter-ner/master/twitter_data/persian-ner-twitter-data/test.txt\n",
        "!wget -q --show-progress https://raw.githubusercontent.com/overfit-ir/persian-twitter-ner/master/twitter_data/persian-ner-twitter-data/dev.txt\n",
        "!mkdir data_twitter && mv train.txt data_twitter && mv test.txt data_twitter && mv dev.txt data_twitter"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train.txt           100%[===================>]   2.20M  --.-KB/s    in 0.06s   \n",
            "test.txt            100%[===================>] 108.10K  --.-KB/s    in 0.01s   \n",
            "dev.txt             100%[===================>] 160.83K  --.-KB/s    in 0.02s   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_Bk-bpscy9l",
        "outputId": "9f67a37e-d021-468c-e9c8-c50ce43ab6cb"
      },
      "source": [
        "!wget -q --show-progress https://raw.githubusercontent.com/overfit-ir/persian-twitter-ner/feature/add-multi-task-learning/ner_data/arman/dev.txt\n",
        "!wget -q --show-progress https://raw.githubusercontent.com/overfit-ir/persian-twitter-ner/feature/add-multi-task-learning/ner_data/arman/train.txt\n",
        "!wget -q --show-progress https://raw.githubusercontent.com/overfit-ir/persian-twitter-ner/feature/add-multi-task-learning/ner_data/arman/test.txt\n",
        "!mkdir data_peyma && mv train.txt data_peyma && mv test.txt data_peyma && mv dev.txt data_peyma"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dev.txt             100%[===================>]   1.77M  --.-KB/s    in 0.04s   \n",
            "train.txt           100%[===================>]   5.37M  --.-KB/s    in 0.1s    \n",
            "test.txt            100%[===================>] 932.44K  --.-KB/s    in 0.04s   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6ssZIt-8i_d"
      },
      "source": [
        "### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Yo1uL4o7FCi",
        "outputId": "c9a303c4-ddd9-4f1c-849d-bf3ffd62c7e5"
      },
      "source": [
        "!pip -q install transformers\n",
        "!pip -q install sentencepiece\n",
        "!pip -q install seqeval\n",
        "!pip -q install conllu"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.0MB 7.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 37.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 52.6MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 7.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 4.6MB/s \n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcikcJHwIMHN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ee3d933a-2faa-4b8a-df2b-74b9fdf42b59"
      },
      "source": [
        "import transformers\n",
        "transformers.__version__"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'4.4.2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ne4kyMz8gBY"
      },
      "source": [
        "### Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Of7QX16O82uF",
        "outputId": "1a66aa57-5327-4e54-c633-eb79baa3d30d"
      },
      "source": [
        "!wget -q --show-progress \"https://raw.githubusercontent.com/stefan-it/fine-tuned-berts-seq/master/scripts/preprocess.py\""
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rpreprocess.py         0%[                    ]       0  --.-KB/s               \rpreprocess.py       100%[===================>]     991  --.-KB/s    in 0s      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcqvEZ-r9frk"
      },
      "source": [
        "#### Preprocess twitter data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWFA3eSk6vxk"
      },
      "source": [
        "!cat data_twitter/train.txt | grep -v \"^#\" | cut -f 1,2 | tr '\\t' ' ' > data_twitter/train.txt.tmp\n",
        "!cat data_twitter/test.txt | grep -v \"^#\" | cut -f 1,2 | tr '\\t' ' ' > data_twitter/test.txt.tmp\n",
        "!cat data_twitter/dev.txt | grep -v \"^#\" | cut -f 1,2 | tr '\\t' ' ' > data_twitter/dev.txt.tmp"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KAjbvS191T5"
      },
      "source": [
        "!mkdir processed_data_twitter"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtGsUfJA7MGm",
        "outputId": "25bc79e9-b0ac-4b4f-ba40-a4d25e7680d8"
      },
      "source": [
        "!python3 preprocess.py data_twitter/dev.txt.tmp HooshvareLab/bert-base-parsbert-uncased 256 > processed_data_twitter/dev.txt\n",
        "!python3 preprocess.py data_twitter/train.txt.tmp HooshvareLab/bert-base-parsbert-uncased 256 > processed_data_twitter/train.txt\n",
        "!python3 preprocess.py data_twitter/test.txt.tmp HooshvareLab/bert-base-parsbert-uncased 256 > processed_data_twitter/test.txt"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: 100% 434/434 [00:00<00:00, 449kB/s]\n",
            "Downloading: 100% 1.22M/1.22M [00:00<00:00, 3.16MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQsAOHeZ9iVQ"
      },
      "source": [
        "#### Preprocess Peyma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLUMBFnI9ldc"
      },
      "source": [
        "!cat data_peyma/train.txt | grep -v \"^#\" | cut -f 1,2 | tr ' ' ' ' > data_peyma/train.txt.tmp\n",
        "!cat data_peyma/test.txt | grep -v \"^#\" | cut -f 1,2 | tr ' ' ' ' > data_peyma/test.txt.tmp\n",
        "!cat data_peyma/dev.txt | grep -v \"^#\" | cut -f 1,2 | tr ' ' ' ' > data_peyma/dev.txt.tmp"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9F3KuFR-QZJ"
      },
      "source": [
        "!mkdir processed_data_peyma"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AynaHLb-TpN"
      },
      "source": [
        "!python3 preprocess.py data_peyma/dev.txt.tmp HooshvareLab/bert-base-parsbert-uncased 256 > processed_data_peyma/dev.txt\n",
        "!python3 preprocess.py data_peyma/train.txt.tmp HooshvareLab/bert-base-parsbert-uncased 256 > processed_data_peyma/train.txt\n",
        "!python3 preprocess.py data_peyma/test.txt.tmp HooshvareLab/bert-base-parsbert-uncased 256 > processed_data_peyma/test.txt"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldFgYj3U8mA5"
      },
      "source": [
        "### Retrieve Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iR4L971T9IVi"
      },
      "source": [
        "!cat processed_data_twitter/train.txt processed_data_twitter/test.txt processed_data_twitter/dev.txt | cut -d \" \" -f 2 | grep -v \"^$\"| sort | uniq > labels_twitter.txt"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NqWmNT3-wLT"
      },
      "source": [
        "!cat processed_data_peyma/train.txt processed_data_peyma/test.txt processed_data_peyma/dev.txt | cut -d \" \" -f 2 | grep -v \"^$\"| sort | uniq > labels_peyma.txt"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STAnAZxi8ri9"
      },
      "source": [
        "### Multi-task Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6s242anr9L7A",
        "outputId": "e41a0432-02dc-4620-aeab-50f569744280"
      },
      "source": [
        "!python3 run_tf_ner.py \\\n",
        "--data_dir_twitter ./processed_data_twitter/ \\\n",
        "--data_dir_peyma ./processed_data_peyma/ \\\n",
        "--labels_twitter labels_twitter.txt \\\n",
        "--labels_peyma labels_peyma.txt \\\n",
        "--model_name_or_path HooshvareLab/bert-base-parsbert-uncased \\\n",
        "--output_dir eval/ \\\n",
        "--max_seq_length_twitter  256 \\\n",
        "--max_seq_length_peyma  256 \\\n",
        "--per_device_train_batch_size 8 \\\n",
        "--save_steps 1000 \\\n",
        "--save_total_limit 1 \\\n",
        "--eval_steps 100 \\\n",
        "--logging_steps 100 \\\n",
        "--logging_first_step True \\\n",
        "--evaluation_strategy steps \\\n",
        "--seed 1 \\\n",
        "--num_train_epochs 5 \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--do_predict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-26 15:21:50.473678: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "[INFO|training_args.py:631] 2021-03-26 15:21:51,640 >> PyTorch: setting up devices\n",
            "[INFO|training_args.py:555] 2021-03-26 15:21:51,703 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "[INFO|training_args_tf.py:192] 2021-03-26 15:21:51,709 >> Tensorflow: setting up strategy\n",
            "2021-03-26 15:21:51.713851: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-03-26 15:21:51.714171: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2021-03-26 15:21:51.714473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-03-26 15:21:51.715080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2021-03-26 15:21:51.715128: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-03-26 15:21:51.787525: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2021-03-26 15:21:51.787633: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-03-26 15:21:51.966213: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2021-03-26 15:21:51.981607: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2021-03-26 15:21:52.265914: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-03-26 15:21:52.279626: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-03-26 15:21:52.284260: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-03-26 15:21:52.284406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-03-26 15:21:52.285154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-03-26 15:21:52.287819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
            "2021-03-26 15:21:52.288637: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-03-26 15:21:52.288780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-03-26 15:21:52.289327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2021-03-26 15:21:52.289397: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-03-26 15:21:52.289429: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2021-03-26 15:21:52.289462: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-03-26 15:21:52.289489: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2021-03-26 15:21:52.289510: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2021-03-26 15:21:52.289531: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-03-26 15:21:52.289555: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-03-26 15:21:52.289579: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-03-26 15:21:52.289652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-03-26 15:21:52.290223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-03-26 15:21:52.290736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
            "2021-03-26 15:21:52.293263: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-03-26 15:21:56.755704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-03-26 15:21:56.755753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
            "2021-03-26 15:21:56.755765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
            "2021-03-26 15:21:56.761146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-03-26 15:21:56.761815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-03-26 15:21:56.762366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-03-26 15:21:56.762872: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-03-26 15:21:56.762921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13994 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "03/26/2021 15:21:56 - INFO - __main__ -   n_replicas: 1, distributed training: False, 16-bits training: False\n",
            "03/26/2021 15:21:56 - INFO - __main__ -   Training/evaluation parameters TFTrainingArguments(output_dir='eval/', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/Mar26_15-21-51_97433d6a444f', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=True, logging_steps=100, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=1000, save_total_limit=1, no_cuda=False, seed=1, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='eval/', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, tpu_name=None, tpu_zone=None, gcp_project=None, poly_power=1.0, xla=False)\n",
            "[INFO|configuration_utils.py:463] 2021-03-26 15:21:56,976 >> loading configuration file https://huggingface.co/HooshvareLab/bert-base-parsbert-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/d3b7c3283a6a4ad4471f59269c9de8adadfab0b05eebf49a64e046fca56cdab2.58cfea678e7bd2c1de3bfd4a5357101526b9fbc32a994b9456047e55b0afbebe\n",
            "[INFO|configuration_utils.py:499] 2021-03-26 15:21:56,977 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"B-EVE\",\n",
            "    \"1\": \"B-LOC\",\n",
            "    \"2\": \"B-NAT\",\n",
            "    \"3\": \"B-ORG\",\n",
            "    \"4\": \"B-PER\",\n",
            "    \"5\": \"B-POG\",\n",
            "    \"6\": \"I-EVE\",\n",
            "    \"7\": \"I-LOC\",\n",
            "    \"8\": \"I-NAT\",\n",
            "    \"9\": \"I-ORG\",\n",
            "    \"10\": \"I-PER\",\n",
            "    \"11\": \"I-POG\",\n",
            "    \"12\": \"O\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B-EVE\": 0,\n",
            "    \"B-LOC\": 1,\n",
            "    \"B-NAT\": 2,\n",
            "    \"B-ORG\": 3,\n",
            "    \"B-PER\": 4,\n",
            "    \"B-POG\": 5,\n",
            "    \"I-EVE\": 6,\n",
            "    \"I-LOC\": 7,\n",
            "    \"I-NAT\": 8,\n",
            "    \"I-ORG\": 9,\n",
            "    \"I-PER\": 10,\n",
            "    \"I-POG\": 11,\n",
            "    \"O\": 12\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.4.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:463] 2021-03-26 15:21:57,183 >> loading configuration file https://huggingface.co/HooshvareLab/bert-base-parsbert-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/d3b7c3283a6a4ad4471f59269c9de8adadfab0b05eebf49a64e046fca56cdab2.58cfea678e7bd2c1de3bfd4a5357101526b9fbc32a994b9456047e55b0afbebe\n",
            "[INFO|configuration_utils.py:499] 2021-03-26 15:21:57,183 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"B-event\",\n",
            "    \"1\": \"B-fac\",\n",
            "    \"2\": \"B-loc\",\n",
            "    \"3\": \"B-org\",\n",
            "    \"4\": \"B-pers\",\n",
            "    \"5\": \"B-pro\",\n",
            "    \"6\": \"I-event\",\n",
            "    \"7\": \"I-fac\",\n",
            "    \"8\": \"I-loc\",\n",
            "    \"9\": \"I-org\",\n",
            "    \"10\": \"I-pers\",\n",
            "    \"11\": \"I-pro\",\n",
            "    \"12\": \"O\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B-event\": 0,\n",
            "    \"B-fac\": 1,\n",
            "    \"B-loc\": 2,\n",
            "    \"B-org\": 3,\n",
            "    \"B-pers\": 4,\n",
            "    \"B-pro\": 5,\n",
            "    \"I-event\": 6,\n",
            "    \"I-fac\": 7,\n",
            "    \"I-loc\": 8,\n",
            "    \"I-org\": 9,\n",
            "    \"I-pers\": 10,\n",
            "    \"I-pro\": 11,\n",
            "    \"O\": 12\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.4.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:463] 2021-03-26 15:21:57,396 >> loading configuration file https://huggingface.co/HooshvareLab/bert-base-parsbert-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/d3b7c3283a6a4ad4471f59269c9de8adadfab0b05eebf49a64e046fca56cdab2.58cfea678e7bd2c1de3bfd4a5357101526b9fbc32a994b9456047e55b0afbebe\n",
            "[INFO|configuration_utils.py:499] 2021-03-26 15:21:57,397 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.4.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-03-26 15:21:58,437 >> loading file https://huggingface.co/HooshvareLab/bert-base-parsbert-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/b80b05f64dc19f3c880b7074ef09108d0bc244e4b6f50d6dba094da0f1c231fd.6699f2ee4745b6531f79b9781879071b6ace2d2768df83889391421fb44d4474\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-03-26 15:21:58,437 >> loading file https://huggingface.co/HooshvareLab/bert-base-parsbert-uncased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-03-26 15:21:58,437 >> loading file https://huggingface.co/HooshvareLab/bert-base-parsbert-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-03-26 15:21:58,437 >> loading file https://huggingface.co/HooshvareLab/bert-base-parsbert-uncased/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1702] 2021-03-26 15:21:58,437 >> loading file https://huggingface.co/HooshvareLab/bert-base-parsbert-uncased/resolve/main/tokenizer.json from cache at None\n",
            "03/26/2021 15:21:58 - INFO - filelock -   Lock 139907028042448 acquired on /root/.cache/huggingface/transformers/9cdad356c7d6956a9814a7cd822941b9a7bc7f3b450273d48f5a6626d97413a4.f8b29776d705d818b1b6d82629b12d40a9867a4c1e55e043dd81c99c2e36a246.h5.lock\n",
            "[INFO|file_utils.py:1386] 2021-03-26 15:21:58,759 >> https://huggingface.co/HooshvareLab/bert-base-parsbert-uncased/resolve/main/tf_model.h5 not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpddh8d68e\n",
            "Downloading: 100% 963M/963M [00:14<00:00, 66.9MB/s]\n",
            "[INFO|file_utils.py:1390] 2021-03-26 15:22:13,559 >> storing https://huggingface.co/HooshvareLab/bert-base-parsbert-uncased/resolve/main/tf_model.h5 in cache at /root/.cache/huggingface/transformers/9cdad356c7d6956a9814a7cd822941b9a7bc7f3b450273d48f5a6626d97413a4.f8b29776d705d818b1b6d82629b12d40a9867a4c1e55e043dd81c99c2e36a246.h5\n",
            "[INFO|file_utils.py:1393] 2021-03-26 15:22:13,559 >> creating metadata file for /root/.cache/huggingface/transformers/9cdad356c7d6956a9814a7cd822941b9a7bc7f3b450273d48f5a6626d97413a4.f8b29776d705d818b1b6d82629b12d40a9867a4c1e55e043dd81c99c2e36a246.h5\n",
            "03/26/2021 15:22:13 - INFO - filelock -   Lock 139907028042448 released on /root/.cache/huggingface/transformers/9cdad356c7d6956a9814a7cd822941b9a7bc7f3b450273d48f5a6626d97413a4.f8b29776d705d818b1b6d82629b12d40a9867a4c1e55e043dd81c99c2e36a246.h5.lock\n",
            "[INFO|modeling_tf_utils.py:1240] 2021-03-26 15:22:13,560 >> loading weights file https://huggingface.co/HooshvareLab/bert-base-parsbert-uncased/resolve/main/tf_model.h5 from cache at /root/.cache/huggingface/transformers/9cdad356c7d6956a9814a7cd822941b9a7bc7f3b450273d48f5a6626d97413a4.f8b29776d705d818b1b6d82629b12d40a9867a4c1e55e043dd81c99c2e36a246.h5\n",
            "2021-03-26 15:22:13.876894: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2021-03-26 15:22:16.414134: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
            "[WARNING|modeling_tf_utils.py:1298] 2021-03-26 15:22:17,345 >> All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_tf_utils.py:1302] 2021-03-26 15:22:17,345 >> Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at HooshvareLab/bert-base-parsbert-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[INFO|modeling_tf_utils.py:1240] 2021-03-26 15:22:17,698 >> loading weights file https://huggingface.co/HooshvareLab/bert-base-parsbert-uncased/resolve/main/tf_model.h5 from cache at /root/.cache/huggingface/transformers/9cdad356c7d6956a9814a7cd822941b9a7bc7f3b450273d48f5a6626d97413a4.f8b29776d705d818b1b6d82629b12d40a9867a4c1e55e043dd81c99c2e36a246.h5\n",
            "[WARNING|modeling_tf_utils.py:1298] 2021-03-26 15:22:18,657 >> All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_tf_utils.py:1302] 2021-03-26 15:22:18,657 >> Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at HooshvareLab/bert-base-parsbert-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "03/26/2021 15:22:19 - INFO - utils_ner -   Writing example 0 of 6417\n",
            "03/26/2021 15:22:19 - INFO - utils_ner -   *** Example ***\n",
            "03/26/2021 15:22:19 - INFO - utils_ner -   guid: train-1\n",
            "03/26/2021 15:22:19 - INFO - utils_ner -   tokens: [CLS] چهارمین قهرمانی فلو ##مین ##نس ##ه در برزیل [SEP]\n",
            "03/26/2021 15:22:19 - INFO - utils_ner -   input_ids: 2 6968 5179 13879 2231 8049 1177 2028 6674 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:22:19 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:22:19 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:22:19 - INFO - utils_ner -   label_ids: -100 12 12 3 -100 -100 -100 12 1 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "03/26/2021 15:22:19 - INFO - utils_ner -   *** Example ***\n",
            "03/26/2021 15:22:19 - INFO - utils_ner -   guid: train-2\n",
            "03/26/2021 15:22:19 - INFO - utils_ner -   tokens: [CLS] زندگی سلام کورا ##سا ##يو بلیز سیشل و نايورو خراسان ورزشی دو بیانضباطی و دو واکنش متفاوت از فرهاد سرخهای پایتخت در اندیشه دبل [SEP]\n",
            "03/26/2021 15:22:19 - INFO - utils_ner -   input_ids: 2 2763 3132 56694 7779 5657 43533 73166 331 51087 4985 4874 2136 34955 331 2136 4104 4215 2036 8723 52933 4840 2028 6793 53073 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:22:19 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:22:19 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:22:19 - INFO - utils_ner -   label_ids: -100 12 12 12 -100 -100 12 12 12 12 3 9 12 12 12 12 12 12 12 4 3 9 12 12 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "03/26/2021 15:22:19 - INFO - utils_ner -   *** Example ***\n",
            "03/26/2021 15:22:19 - INFO - utils_ner -   guid: train-3\n",
            "03/26/2021 15:22:19 - INFO - utils_ner -   tokens: [CLS] سال [UNK] را سالی که در ان بایرن همه جامهای ممکن را برد و حالا [UNK] تکرار کسب تمامی عناوین ممکن لیگ قهرمانان بوندس لیگا جام حذفی المان سوپرکاپ اروپا ( سال [UNK] قهرمان جام باشگاههای جهان ) [SEP]\n",
            "03/26/2021 15:22:19 - INFO - utils_ner -   input_ids: 2 2101 1 2049 5622 2046 2028 2050 12028 2440 29809 3085 2049 3736 331 3826 1 5439 3161 3629 9337 3085 3820 7112 27507 25372 2533 10236 4016 43671 3509 9 2101 1 6000 2533 9489 2685 10 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:22:19 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:22:19 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:22:19 - INFO - utils_ner -   label_ids: -100 12 12 12 12 12 12 12 3 12 12 12 12 12 12 12 12 12 12 12 12 12 0 6 0 6 0 6 6 0 6 12 12 12 0 6 6 6 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "03/26/2021 15:22:19 - INFO - utils_ner -   *** Example ***\n",
            "03/26/2021 15:22:19 - INFO - utils_ner -   guid: train-4\n",
            "03/26/2021 15:22:19 - INFO - utils_ner -   tokens: [CLS] اگه از هر قریه کشور یک ازمایش dna گرفته شود باز خاد دیدیم که نه پشتون پشتون است نه تاجیک تاجیک فخر قومی و ملیتی قصه کچ ##ری قروت خاد شد مط ##من استم تقریبا ده درصد حشمت غنی هم هزاره است مثل یک سومالیایی سیا پوست ده یوتیوب نشان میداد که ده درصد چین ##ایی است دراصل از خاکی ##م و در خاکی ##م [SEP]\n",
            "03/26/2021 15:22:19 - INFO - utils_ner -   input_ids: 2 12915 2036 2202 37139 2116 2076 4764 17023 2563 2268 2266 11138 10725 2046 2505 55145 55145 2045 2505 10444 10444 14935 13097 331 24339 8468 21727 2059 94422 11138 2087 2294 2316 9661 4557 2326 2361 25549 6648 2063 16365 2045 3452 2076 52621 10117 4531 2326 15916 2555 4861 2046 2326 2361 3443 2100 2045 75108 2036 12538 1155 331 2028 12538 1155 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:22:19 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:22:19 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:22:19 - INFO - utils_ner -   label_ids: -100 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 2 2 12 12 2 2 12 12 12 12 12 12 -100 12 12 12 12 -100 12 12 12 12 12 12 12 12 12 12 12 2 12 12 12 3 12 12 12 12 12 12 -100 12 12 12 12 -100 12 12 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "03/26/2021 15:22:19 - INFO - utils_ner -   *** Example ***\n",
            "03/26/2021 15:22:19 - INFO - utils_ner -   guid: train-5\n",
            "03/26/2021 15:22:19 - INFO - utils_ner -   tokens: [CLS] اختلافات سپاه تهران و فرماندهی سپاه انکار ##شدنی نیست نکته این است که برای طرفین ماجرا این یک اختلاف درون خانواده انقلاب و سپاه است حالا bbc هم مرده ##خوری میکند وگرنه رسانه ملکه و سربازان فارسیزبان ##ش با جنگ و سپاه مشکل ریشهای دارند از فتحالمبین و بیتالمقدس که همه یکدست بودند [SEP]\n",
            "03/26/2021 15:22:19 - INFO - utils_ner -   input_ids: 2 7329 4008 2402 331 7032 4008 7108 15891 2380 3926 2042 2045 2046 2073 8015 5940 2042 2076 4201 4710 3045 2858 331 4008 2045 3826 14208 2063 11627 11883 2313 11874 3080 12352 331 8370 30760 1176 2037 2941 331 4008 2524 15983 2522 2036 54398 331 24449 2046 2440 19160 2820 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:22:19 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:22:19 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:22:19 - INFO - utils_ner -   label_ids: -100 12 3 9 12 12 3 12 -100 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 3 12 12 3 12 12 -100 12 12 12 4 12 12 12 -100 12 12 12 3 12 12 12 12 0 12 0 12 12 12 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "03/26/2021 15:22:32 - INFO - utils_ner -   Writing example 0 of 15362\n",
            "03/26/2021 15:22:32 - INFO - utils_ner -   *** Example ***\n",
            "03/26/2021 15:22:32 - INFO - utils_ner -   guid: train-1\n",
            "03/26/2021 15:22:32 - INFO - utils_ner -   tokens: [CLS] به عنوان مثال وقتی نشریات مدافع اصول و ارزشها و منادی انقلاب و اسلام در بالاترین درجه ، اولین و درشتترین تیتر نشریه خود را در صدر صفحه نخستین ، به تکذیب اظهارات و نظریات مشاور ريیسجمهور با همین ترکیب عبارتی و البته از قول دیگران اختصاص میدهند ، ایا در موارد مشابه نیز هر گاه خبر تکذیب متوجه و معطوف به شخصی باشد [UNK] در زمره مشاوران [UNK] مقام بلندمرتبه ##ی دیگر است ، خبر را عینا به همین درشتی و با همین ترکیب عبارتی در صدر صفحه نخست به چاپ میرساند و در ان مورد هم به جای ذکر نام ي ##ا عضویت ان شخص در گروه و کمیتهی خاص صرفا بر روی عنوان مشاور فلان مسيول بلندمرتبه تاکید میکنند ؟ اگر نشریه دیگری چنین کند ، شدیدا مورد انتقاد انان قرار نمیگیرد [UNK] صورت خبر را بهانه کرده و موضوع دیگری را هدف قرار داده است ؟ بااینهمه ، بازهم از قضاوت قطعی پرهیز کرده ، میگويیم فقط نتیجه مقایسه و تطبیق در [UNK] وضعیت جامعال ##اطراف نشان خواهد داد [UNK] رسانههای جبههی منتقد و مخالف ريیسجمهور و دولت در مقام نقادی واقعا یکسان و بدون تبعیض عمل کردهاند ي ##ا خدایناکرده ولو در عالم فرض به عنوان مثال غیر دولت و غیر ريیسجمهور را با نوازش ##های لسانی و قلمی نقد کردهاند اما دولت و ريیسجمهور را با خمپارههای کلامی و نوشتاری هدف نقادی ##های موسوم به نقد خیرخواهانه قرار دادهاند . [SEP]\n",
            "03/26/2021 15:22:32 - INFO - utils_ner -   input_ids: 2 2031 2339 4281 3043 11084 5966 3655 331 11751 331 30672 2858 331 2393 2028 6204 4817 300 3001 331 94571 9126 7446 2081 2049 2028 5354 4203 3633 300 2031 7302 4819 331 17727 3753 4962 2037 2531 4950 9410 331 2725 2036 5860 4425 3473 3467 300 3174 2028 3278 4312 2174 2202 5851 2165 7302 4532 331 10282 2031 4343 2302 1 2028 12632 7769 1 2882 22034 1158 2263 2045 300 2165 2049 9425 2031 2531 33809 331 2037 2531 4950 9410 2028 5354 4203 2894 2031 3552 8144 331 2028 2050 2334 2063 2031 2585 4546 2410 333 1157 4793 2050 2983 2028 2690 331 40334 3154 5263 2043 2421 2339 3753 9984 2900 22034 2617 2484 303 2329 7446 2953 2978 2299 300 14419 2334 3961 3609 2232 9709 1 2353 2165 2049 6370 2224 331 2534 2953 2049 2865 2232 2488 2045 303 73919 300 10463 2036 7175 5629 7776 2224 300 44495 3171 3081 4235 331 9914 2028 1 2920 88606 56879 2555 2332 2197 1 4734 40661 5770 331 4118 4962 331 2237 2028 2882 39978 5134 7066 331 2839 9875 2684 3115 333 1157 95145 11542 2028 7742 6588 2031 2339 4281 2508 2237 331 2508 4962 2049 2037 26595 2039 48131 331 31599 5005 3115 2195 2237 331 4962 2049 2037 84718 14086 331 21268 2865 39978 2039 7585 2031 5005 23859 2232 4786 15 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:22:32 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:22:32 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:22:32 - INFO - utils_ner -   label_ids: -100 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 -100 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 -100 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 -100 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 -100 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 -100 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 -100 12 12 12 12 12 12 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "03/26/2021 15:22:32 - INFO - utils_ner -   *** Example ***\n",
            "03/26/2021 15:22:32 - INFO - utils_ner -   guid: train-2\n",
            "03/26/2021 15:22:32 - INFO - utils_ner -   tokens: [CLS] دکتر اصغری دبیر چهارمین همایش انجمن زمینشناسی ایران در این زمینه گفت : از مجموع چهار صد مقاله رسیده به دبیرخانه همایش ، [UNK] صد و هشتاد مقاله ظرف مدت دو روز در هشت سالن همایش برگزار شد . [SEP]\n",
            "03/26/2021 15:22:32 - INFO - utils_ner -   input_ids: 2 3216 20043 3642 6968 4435 3927 20777 2119 2028 2042 2754 2117 17 2036 3935 2634 2914 6899 3364 2031 8085 4435 300 1 2914 331 12378 6899 3152 2893 2136 2182 2028 3776 5553 4435 2877 2087 15 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:22:32 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:22:32 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:22:32 - INFO - utils_ner -   label_ids: -100 12 4 12 0 6 6 6 6 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "03/26/2021 15:22:32 - INFO - utils_ner -   *** Example ***\n",
            "03/26/2021 15:22:32 - INFO - utils_ner -   guid: train-3\n",
            "03/26/2021 15:22:32 - INFO - utils_ner -   tokens: [CLS] دکتر اکبر میرع ##رب در همایش بررسی و پیشگیری از بیماری ایدز در همدان به خبرنگاران گفت : کمیتهای در سطح استان شامل اموزش نیروی انسانی ، کنترل فراوردههای خونی ، برقراری نظام گزارشدهی و اموزش دانشاموزان دختر و پسر در مقطع سوم راهنمايی برای پیشگیری از این بیماری تشکیل شده است . [SEP]\n",
            "03/26/2021 15:22:32 - INFO - utils_ner -   input_ids: 2 3216 6706 45135 2663 2028 4435 2712 331 5098 2036 3158 10007 2028 6459 2031 4441 2117 17 15014 2028 2952 2503 3438 2976 3652 4234 300 3434 6032 9147 300 6044 2834 40784 331 2976 8028 4589 331 5071 2028 6567 3310 54313 2073 5098 2036 2042 3158 3211 2110 2045 15 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:22:32 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:22:32 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:22:32 - INFO - utils_ner -   label_ids: -100 12 4 10 -100 12 0 6 6 6 6 6 6 12 2 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "03/26/2021 15:22:32 - INFO - utils_ner -   *** Example ***\n",
            "03/26/2021 15:22:32 - INFO - utils_ner -   guid: train-4\n",
            "03/26/2021 15:22:32 - INFO - utils_ner -   tokens: [CLS] اردبیل [UNK] استاندار اردبیل گفت : به مناسبت هفته دولت [UNK] طرح عمرانی و تولیدی در مناطق روستايی و شهری این استان به بهرهبرداری میرسد . [SEP]\n",
            "03/26/2021 15:22:32 - INFO - utils_ner -   input_ids: 2 7340 1 6515 7340 2117 17 2031 5924 2759 2237 1 2501 6561 331 4426 2028 3368 25640 331 3046 2042 2503 2031 6267 3179 15 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:22:32 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:22:32 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:22:32 - INFO - utils_ner -   label_ids: -100 2 12 12 2 12 12 12 12 0 6 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "03/26/2021 15:22:32 - INFO - utils_ner -   *** Example ***\n",
            "03/26/2021 15:22:32 - INFO - utils_ner -   guid: train-5\n",
            "03/26/2021 15:22:32 - INFO - utils_ner -   tokens: [CLS] حمید طاهایی افزود : برای اجرای این طرحها [UNK] میلیارد و [UNK] میلیون ریال اعتبار هزینه شده است . [SEP]\n",
            "03/26/2021 15:22:32 - INFO - utils_ner -   input_ids: 2 5402 68221 2441 17 2073 2810 2042 7886 1 2613 331 1 2457 3185 3421 3145 2110 2045 15 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:22:32 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:22:32 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:22:32 - INFO - utils_ner -   label_ids: -100 4 10 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "03/26/2021 15:22:52 - INFO - utils_ner -   Writing example 10000 of 15362\n",
            "03/26/2021 15:23:02 - INFO - utils_ner -   Writing example 0 of 446\n",
            "03/26/2021 15:23:02 - INFO - utils_ner -   *** Example ***\n",
            "03/26/2021 15:23:02 - INFO - utils_ner -   guid: dev-1\n",
            "03/26/2021 15:23:02 - INFO - utils_ner -   tokens: [CLS] باید از امید اروپاییها برید مخصوصا ایرانیانی که در پارلمانها هستند و طوری وانمود میکنند که رژیم ایران یک رژیم عادی ولی سختگیر است [SEP]\n",
            "03/26/2021 15:23:02 - INFO - utils_ner -   input_ids: 2 2212 2036 2878 13864 22242 8779 33874 2046 2028 66309 2373 331 4624 19439 2484 2046 3958 2119 2076 3958 5168 2752 24199 2045 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:23:02 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:23:02 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:23:02 - INFO - utils_ner -   label_ids: -100 12 12 12 12 12 12 2 12 12 12 12 12 12 12 12 12 12 1 12 12 12 12 12 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "03/26/2021 15:23:02 - INFO - utils_ner -   *** Example ***\n",
            "03/26/2021 15:23:02 - INFO - utils_ner -   guid: dev-2\n",
            "03/26/2021 15:23:02 - INFO - utils_ner -   tokens: [CLS] پیدیاف اقتصاد سیاسی نیکی ##تین پیدا میشه سینا [SEP]\n",
            "03/26/2021 15:23:02 - INFO - utils_ner -   input_ids: 2 64971 2529 2809 12466 9847 2880 11290 8588 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:23:02 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:23:02 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:23:02 - INFO - utils_ner -   label_ids: -100 12 12 12 12 -100 12 12 4 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "03/26/2021 15:23:02 - INFO - utils_ner -   *** Example ***\n",
            "03/26/2021 15:23:02 - INFO - utils_ner -   guid: dev-3\n",
            "03/26/2021 15:23:02 - INFO - utils_ner -   tokens: [CLS] ذهنم به گوشههای دورتر ان بران ##م از همان کورس ( حالا به نام کاج ) اکنون یک دختر اول نمره عمومی کانک ##ور شده است یاد ان روز شوم و شام غربت در خاطرم زنده شد شمسی ##ه نشان داد که خم میشویم اما نمیش ##کنیم [UNK] [SEP]\n",
            "03/26/2021 15:23:02 - INFO - utils_ner -   input_ids: 2 18251 2031 28171 17432 2050 14648 1155 2036 2745 20921 9 3826 2031 2410 22605 10 3113 2076 4589 2389 8558 2899 21347 2034 2110 2045 2786 2050 2182 7834 331 7428 23200 2028 27099 4797 2087 11364 1177 2555 2197 2046 8097 7333 2195 7708 19493 1 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:23:02 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:23:02 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:23:02 - INFO - utils_ner -   label_ids: -100 12 12 12 12 12 12 -100 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 -100 12 12 12 12 12 12 12 12 12 12 12 12 12 4 -100 12 12 12 12 12 12 12 -100 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "03/26/2021 15:23:02 - INFO - utils_ner -   *** Example ***\n",
            "03/26/2021 15:23:02 - INFO - utils_ner -   guid: dev-4\n",
            "03/26/2021 15:23:02 - INFO - utils_ner -   tokens: [CLS] بیمه بیکاری حقی ##ه ##که بهتر از من میدونین هر هفته از پیرو ##ل کسر میشه و هیچ سنخیتی با ولف ##ر که کمک هزینه مستمندان هستش نداره ولی در کل موافقم وقتی میتونی کار کنی و مالیات بدی بهتره از این کمک هزینهها دور باشی [SEP]\n",
            "03/26/2021 15:23:02 - INFO - utils_ner -   input_ids: 2 2785 5231 15516 1177 2133 3446 2036 2078 87587 2202 2759 2036 11048 1173 8662 11290 331 2608 35821 2037 23921 1156 2046 2817 3145 38238 31540 18803 2752 2028 2381 27082 3043 36803 2109 8374 331 4325 6339 30664 2036 2042 2817 7748 2394 12537 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:23:02 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:23:02 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:23:02 - INFO - utils_ner -   label_ids: -100 12 12 12 -100 -100 12 12 12 12 12 12 12 12 -100 12 12 12 12 12 12 12 -100 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "03/26/2021 15:23:02 - INFO - utils_ner -   *** Example ***\n",
            "03/26/2021 15:23:02 - INFO - utils_ner -   guid: dev-5\n",
            "03/26/2021 15:23:02 - INFO - utils_ner -   tokens: [CLS] ششم [UNK] ) در کوچههای سرش ##ور ول میگشتم که صدای روضهای حزن ##انگیز را شنیدم دل به دریا زدم و وارد گودی کرونا شدم به عمرم ندیده بودم که کسی چنین سوزی را برای زندگی سر داده باشد نه در سوفو ##کل نه در شکسپیر و نه در گوته [SEP]\n",
            "03/26/2021 15:23:02 - INFO - utils_ner -   input_ids: 2 4969 1 10 2028 20948 10969 2034 3891 46367 2046 4873 84885 35855 11297 2049 13999 2420 2031 2628 12411 331 2480 35190 28114 5564 2031 27042 12367 4703 2046 3256 2978 8544 2049 2073 2763 2140 2488 2302 2505 2028 97609 2262 2505 2028 27500 331 2505 2028 42754 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:23:02 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:23:02 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:23:02 - INFO - utils_ner -   label_ids: -100 12 12 12 12 12 12 -100 12 12 12 12 12 12 -100 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 4 -100 12 12 4 12 12 12 4 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "03/26/2021 15:23:03 - INFO - utils_ner -   Writing example 0 of 5121\n",
            "03/26/2021 15:23:03 - INFO - utils_ner -   *** Example ***\n",
            "03/26/2021 15:23:03 - INFO - utils_ner -   guid: dev-1\n",
            "03/26/2021 15:23:03 - INFO - utils_ner -   tokens: [CLS] افقی : [UNK] [UNK] از عوامل دوران پهلوی و نخستوزیر ایران در سالهای ابتدايی دهه چهل خورشیدی [UNK] جلد سوم یادداشتهای ##ش هم چندی پیش در تهران منتشر شد [UNK] [UNK] پرستاری از ناخوش ##احوال [UNK] پوشاک و جامه [UNK] فانتزی و شیک [UNK] [UNK] در حال وزیدن [UNK] اطلاعیه [UNK] پایتخت جمهوری استونی در حوضه بالتیک [UNK] [UNK] علم راهبرد موسسه و سازمان [UNK] نوعی شمع [UNK] [UNK] حرف جمع مونث [UNK] در ایران به تولیدکننده کتاب اطلاق میشود [UNK] از شهرهای باختری افغانستان [UNK] تا عصر ناصرالدینشاه جزيی از خراسان بود [UNK] ویتامین انعقاد [UNK] [UNK] سبزی غدهای [UNK] دوستی و محبت [UNK] داستان بلند [UNK] شهری در المان [UNK] [UNK] سلول بدن موجودات [UNK] از انواع کالباس [UNK] [UNK] حاشیه و هامش [UNK] پیدا نشدنی [UNK] خرگوش تازی [UNK] [UNK] کنایه از جمعاوری افراطی مال و اموال است [UNK] تنبل و تن ##پرور [UNK] [UNK] حرف پرسش [UNK] پایتخت کشور جزیرهای نايورو در اقیانوسیه [UNK] جنس خشن [UNK] ساز زهی ایرانی [UNK] [UNK] در جاهلیت ان را میپرست ##یدند [UNK] حاجت و مقصود [UNK] زیرا و برای [UNK] نفی تازی [UNK] [UNK] دشت پهناور و مرتفع در اصطلاح جغرافیايی [UNK] تار عنکبوت [UNK] [UNK] دوستان و رفقا [UNK] منافق و مزدور [UNK] مسافر سرزمین عج ##ايب [UNK] [UNK] پرسش [UNK] خط مایل و اریب [UNK] رودی است در یوگسلاوی [UNK] [UNK] از معروفترین داستانهای تخیلی ژول ورن نویسنده فرانسوی [UNK] تاکنون چندین فیلم و سریال را [UNK] بر اساس ان ساختهاند . [SEP]\n",
            "03/26/2021 15:23:03 - INFO - utils_ner -   input_ids: 2 15316 17 1 1 2036 4053 3595 8651 331 9830 2119 2028 3205 46096 3985 8764 7804 1 9552 3310 19806 1176 2063 6530 2226 2028 2402 2979 2087 1 1 12060 2036 14981 50274 1 8043 331 16013 1 16711 331 15400 1 1 2028 2290 47153 1 8928 1 4840 2691 23321 2028 15379 32479 1 1 4836 7903 3914 331 2358 1 4598 19203 1 1 3163 3571 32791 1 2028 2119 2031 6620 3018 13553 2167 1 2036 4704 15701 4981 1 2093 3400 37801 4792 2036 4985 2083 1 5567 9252 1 1 6003 56416 1 6599 331 9091 1 4556 3929 1 3046 2028 4016 1 1 9036 3532 12510 1 2036 3827 19462 1 1 4320 331 70236 1 2880 17792 1 27996 25424 1 1 15619 2036 7996 7730 3882 331 6185 2045 1 21840 331 2312 20720 1 1 3163 5522 1 4840 2116 18595 51087 2028 15366 1 5744 9834 1 2221 24703 2732 1 1 2028 28727 2050 2049 64438 7292 1 29211 331 13526 1 3561 331 2073 1 12940 25424 1 1 9297 23848 331 10908 2028 6680 55832 1 8380 23328 1 1 4603 331 41518 1 9429 331 23282 1 6018 5969 6919 16115 1 1 5522 1 2624 8590 331 68405 1 26889 2045 2028 27733 1 1 2036 13523 11907 16665 32722 49668 5737 6136 1 3573 4688 2627 331 4680 2049 1 2043 2677 2050 16356 15 4 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:23:03 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:23:03 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:23:03 - INFO - utils_ner -   label_ids: -100 12 12 12 12 12 12 12 12 12 12 2 12 12 12 12 12 12 12 12 12 12 -100 12 12 12 12 2 12 12 12 12 12 12 12 -100 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 2 8 8 8 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 2 12 12 12 12 12 12 12 12 12 2 12 12 12 4 12 12 2 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 2 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 -100 12 12 12 12 12 12 12 12 2 12 2 12 12 12 12 12 12 3 12 12 12 12 12 12 12 -100 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 -100 12 12 12 12 12 12 12 12 12 12 12 12 2 12 12 12 12 12 12 4 10 12 3 12 12 12 12 12 12 12 12 12 12 12 12 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "03/26/2021 15:23:03 - INFO - utils_ner -   *** Example ***\n",
            "03/26/2021 15:23:03 - INFO - utils_ner -   guid: dev-2\n",
            "03/26/2021 15:23:03 - INFO - utils_ner -   tokens: [CLS] طاهایی گفت : [UNK] طرح عمرانی ، [UNK] طرح مدرسهسازی در قالب [UNK] کلاس درس ، پنج طرح مسکن و چند طرح فرهنگی دیگر از جمله طرحها ##يی است [UNK] در هفته دولت مورد بهرهبرداری قرار میگیرند . [SEP]\n",
            "03/26/2021 15:23:03 - INFO - utils_ner -   input_ids: 2 68221 2117 17 1 2501 6561 300 1 2501 54914 2028 4249 1 5588 5404 300 2795 2501 2794 331 2429 2501 2968 2263 2036 2874 7886 3640 2045 1 2028 2759 2237 2334 6267 2232 4876 15 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:23:03 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:23:03 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:23:03 - INFO - utils_ner -   label_ids: -100 4 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 -100 12 12 12 0 6 12 12 12 12 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "03/26/2021 15:23:03 - INFO - utils_ner -   *** Example ***\n",
            "03/26/2021 15:23:03 - INFO - utils_ner -   guid: dev-3\n",
            "03/26/2021 15:23:03 - INFO - utils_ner -   tokens: [CLS] و ##ي افزود : از ان جا [UNK] منطقه یادشده به عنوان [UNK] نقطه ارتباطی میان فرهنگهای پیش از تاریخ چهارمحال و بختیاری و جلگه مرودشت از [UNK] سو و فلات مرکزی از سوی دیگر تلقی میشود ، لزوم مطالعات فرهنگهای پیش از تاریخ در این زمینه از اهمیت خاصی برخوردار است . [SEP]\n",
            "03/26/2021 15:23:03 - INFO - utils_ner -   input_ids: 2 331 1216 2441 17 2036 2050 3728 1 2692 11558 2031 2339 1 4660 5556 2630 18872 2226 2036 2251 11299 331 8957 331 33564 27964 2036 1 2424 331 16586 2721 2036 2629 2263 6733 2167 300 5574 5368 18872 2226 2036 2251 2028 2042 2754 2036 3631 4502 4028 2045 15 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:23:03 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:23:03 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:23:03 - INFO - utils_ner -   label_ids: -100 12 -100 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 2 8 8 12 2 8 12 12 12 12 2 8 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "03/26/2021 15:23:03 - INFO - utils_ner -   *** Example ***\n",
            "03/26/2021 15:23:03 - INFO - utils_ner -   guid: dev-4\n",
            "03/26/2021 15:23:03 - INFO - utils_ner -   tokens: [CLS] هیات باستانی منطقه سمیرم همچنین موفق به کشف [UNK] کتیبه پهلوی ساسانی در این منطقه شد . [SEP]\n",
            "03/26/2021 15:23:03 - INFO - utils_ner -   input_ids: 2 3125 9617 2692 34801 2435 3003 2031 4628 1 19229 8651 17029 2028 2042 2692 2087 15 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:23:03 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:23:03 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:23:03 - INFO - utils_ner -   label_ids: -100 12 12 2 8 12 12 12 12 12 12 12 12 12 12 12 12 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "03/26/2021 15:23:03 - INFO - utils_ner -   *** Example ***\n",
            "03/26/2021 15:23:03 - INFO - utils_ner -   guid: dev-5\n",
            "03/26/2021 15:23:03 - INFO - utils_ner -   tokens: [CLS] شهرضا [UNK] وزیر نیرو ، زمان بهرهبرداری از طرح تامین اب اشامیدنی شهرستان شهرضا از توابع استان اصفهان را سال اینده اعلام کرد . [SEP]\n",
            "03/26/2021 15:23:03 - INFO - utils_ner -   input_ids: 2 29206 1 2485 2883 300 2571 6267 2036 2501 2780 2277 12577 3500 29206 2036 10540 2503 4132 2049 2101 2806 2405 2068 15 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:23:03 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:23:03 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/26/2021 15:23:03 - INFO - utils_ner -   label_ids: -100 2 12 12 3 12 12 12 12 12 12 12 12 2 8 12 12 2 8 12 12 12 12 12 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "[INFO|trainer_tf.py:117] 2021-03-26 15:23:13,891 >> You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n",
            "[INFO|trainer_tf.py:125] 2021-03-26 15:23:13,891 >> To use comet_ml logging, run `pip/conda install comet_ml` see https://www.comet.ml/docs/python-sdk/huggingface/\n",
            "2021-03-26 15:23:13.922007: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:23:13.922061: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:23:13.922564: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8695\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "2021-03-26 15:23:13.999607: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:23:13.999666: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:23:13.999785: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8724\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:528] 2021-03-26 15:23:14,004 >> ***** Running training *****\n",
            "[INFO|trainer_tf.py:529] 2021-03-26 15:23:14,004 >>   Num examples = 21779\n",
            "[INFO|trainer_tf.py:531] 2021-03-26 15:23:14,004 >>   Num Epochs = 5\n",
            "[INFO|trainer_tf.py:532] 2021-03-26 15:23:14,004 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer_tf.py:534] 2021-03-26 15:23:14,004 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer_tf.py:536] 2021-03-26 15:23:14,005 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer_tf.py:537] 2021-03-26 15:23:14,005 >>   Steps per epoch = 2723\n",
            "[INFO|trainer_tf.py:538] 2021-03-26 15:23:14,005 >>   Total optimization steps = 13615\n",
            "2021-03-26 15:23:14.047743: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
            "2021-03-26 15:23:14.049814: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2199995000 Hz\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "03/26/2021 15:23:20 - WARNING - tensorflow -   The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "03/26/2021 15:23:21 - WARNING - tensorflow -   The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:390: UserWarning: Using `-1` to mask the loss for the token is deprecated. Please use `-100` instead.\n",
            "  return py_builtins.overload_of(f)(*args)\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "03/26/2021 15:23:26 - WARNING - tensorflow -   The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "03/26/2021 15:23:26 - WARNING - tensorflow -   The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:23:33,419 >> {'loss': 2.7909265, 'learning_rate': 4.9996328e-05, 'epoch': 0.0003672420124862284, 'step': 1}\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "03/26/2021 15:23:35 - WARNING - tensorflow -   The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "03/26/2021 15:23:35 - WARNING - tensorflow -   The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "2021-03-26 15:24:24.452911: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:24:24.453108: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:24:24 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:24:24 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:24:24 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:24:24 - INFO - utils_ner -     Batch size = 8\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "03/26/2021 15:24:24 - WARNING - tensorflow -   The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "03/26/2021 15:24:24 - WARNING - tensorflow -   The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:24:34,044 >> {'eval_loss': 0.18277582100459508, 'eval_precision': 0.6163141993957704, 'eval_recall': 0.6, 'eval_f1': 0.6080476900149032, 'epoch': 0.03672420124862284, 'step': 100}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:24:34,048 >> {'loss': 0.42404166, 'learning_rate': 4.9632756e-05, 'epoch': 0.03672420124862284, 'step': 100}\n",
            "2021-03-26 15:25:19.260472: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:25:19.260651: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:25:19 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:25:19 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:25:19 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:25:19 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:25:27,830 >> {'eval_loss': 0.14120313950947352, 'eval_precision': 0.7082494969818913, 'eval_recall': 0.6901960784313725, 'eval_f1': 0.6991062562065541, 'epoch': 0.07344840249724569, 'step': 200}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:25:27,834 >> {'loss': 0.29908913, 'learning_rate': 4.9265513e-05, 'epoch': 0.07344840249724569, 'step': 200}\n",
            "2021-03-26 15:26:12.631708: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:26:12.631877: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:26:12 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:26:12 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:26:12 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:26:12 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:26:20,823 >> {'eval_loss': 0.12934603009905135, 'eval_precision': 0.7296511627906976, 'eval_recall': 0.7382352941176471, 'eval_f1': 0.7339181286549706, 'epoch': 0.11017260374586853, 'step': 300}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:26:20,826 >> {'loss': 0.24504326, 'learning_rate': 4.889827e-05, 'epoch': 0.11017260374586853, 'step': 300}\n",
            "2021-03-26 15:27:05.826911: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:27:05.827084: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:27:05 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:27:05 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:27:05 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:27:05 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:27:13,964 >> {'eval_loss': 0.11411544254847936, 'eval_precision': 0.7209737827715356, 'eval_recall': 0.7549019607843137, 'eval_f1': 0.7375478927203065, 'epoch': 0.14689680499449137, 'step': 400}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:27:13,967 >> {'loss': 0.21164072, 'learning_rate': 4.853103e-05, 'epoch': 0.14689680499449137, 'step': 400}\n",
            "2021-03-26 15:27:58.979191: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:27:58.979365: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:27:58 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:27:58 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:27:58 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:27:58 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:28:07,151 >> {'eval_loss': 0.1189192703792027, 'eval_precision': 0.7404942965779467, 'eval_recall': 0.7637254901960784, 'eval_f1': 0.7519305019305019, 'epoch': 0.1836210062431142, 'step': 500}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:28:07,155 >> {'loss': 0.19111547, 'learning_rate': 4.8163787e-05, 'epoch': 0.1836210062431142, 'step': 500}\n",
            "2021-03-26 15:28:52.175215: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:28:52.175396: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:28:52 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:28:52 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:28:52 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:28:52 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:29:00,342 >> {'eval_loss': 0.12455895968845912, 'eval_precision': 0.7135183527305282, 'eval_recall': 0.7813725490196078, 'eval_f1': 0.745905474964904, 'epoch': 0.22034520749173706, 'step': 600}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:29:00,345 >> {'loss': 0.17690961, 'learning_rate': 4.7796548e-05, 'epoch': 0.22034520749173706, 'step': 600}\n",
            "2021-03-26 15:29:45.269143: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:29:45.269341: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:29:45 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:29:45 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:29:45 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:29:45 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:29:53,437 >> {'eval_loss': 0.13291497741426742, 'eval_precision': 0.6723856209150327, 'eval_recall': 0.8068627450980392, 'eval_f1': 0.733511586452763, 'epoch': 0.2570694087403599, 'step': 700}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:29:53,440 >> {'loss': 0.16395937, 'learning_rate': 4.74293e-05, 'epoch': 0.2570694087403599, 'step': 700}\n",
            "2021-03-26 15:30:38.337338: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:30:38.337513: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:30:38 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:30:38 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:30:38 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:30:38 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:30:46,493 >> {'eval_loss': 0.11235844237463814, 'eval_precision': 0.7539315448658649, 'eval_recall': 0.7990196078431373, 'eval_f1': 0.7758210376011423, 'epoch': 0.29379360998898274, 'step': 800}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:30:46,496 >> {'loss': 0.15667161, 'learning_rate': 4.706206e-05, 'epoch': 0.29379360998898274, 'step': 800}\n",
            "2021-03-26 15:31:31.442507: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:31:31.442675: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:31:31 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:31:31 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:31:31 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:31:31 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:31:39,604 >> {'eval_loss': 0.11438935143607003, 'eval_precision': 0.7277533039647577, 'eval_recall': 0.8098039215686275, 'eval_f1': 0.7665893271461717, 'epoch': 0.3305178112376056, 'step': 900}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:31:39,607 >> {'loss': 0.14801447, 'learning_rate': 4.669482e-05, 'epoch': 0.3305178112376056, 'step': 900}\n",
            "2021-03-26 15:32:24.597129: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:32:24.597299: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:32:24 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:32:24 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:32:24 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:32:24 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:32:32,760 >> {'eval_loss': 0.1085377846445356, 'eval_precision': 0.7458563535911602, 'eval_recall': 0.7941176470588235, 'eval_f1': 0.7692307692307693, 'epoch': 0.3672420124862284, 'step': 1000}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:32:32,763 >> {'loss': 0.1428135, 'learning_rate': 4.632758e-05, 'epoch': 0.3672420124862284, 'step': 1000}\n",
            "[INFO|trainer_tf.py:595] 2021-03-26 15:32:53,034 >> Saving checkpoint for step 1000 at eval/checkpoint/ckpt-1\n",
            "2021-03-26 15:33:38.332380: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:33:38.332550: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:33:38 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:33:38 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:33:38 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:33:38 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:33:46,518 >> {'eval_loss': 0.10191109350749425, 'eval_precision': 0.7678227360308285, 'eval_recall': 0.7813725490196078, 'eval_f1': 0.7745383867832848, 'epoch': 0.40396621373485125, 'step': 1100}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:33:46,521 >> {'loss': 0.13815649, 'learning_rate': 4.5960336e-05, 'epoch': 0.40396621373485125, 'step': 1100}\n",
            "2021-03-26 15:34:31.332139: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:34:31.332316: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:34:31 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:34:31 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:34:31 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:34:31 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:34:39,506 >> {'eval_loss': 0.10474259512765068, 'eval_precision': 0.7544517338331771, 'eval_recall': 0.7892156862745098, 'eval_f1': 0.7714422616195494, 'epoch': 0.4406904149834741, 'step': 1200}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:34:39,509 >> {'loss': 0.132941, 'learning_rate': 4.5593093e-05, 'epoch': 0.4406904149834741, 'step': 1200}\n",
            "2021-03-26 15:35:24.505768: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:35:24.505941: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:35:24 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:35:24 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:35:24 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:35:24 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:35:32,656 >> {'eval_loss': 0.10246715375355311, 'eval_precision': 0.7670083876980429, 'eval_recall': 0.8068627450980392, 'eval_f1': 0.7864309603440038, 'epoch': 0.47741461623209697, 'step': 1300}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:35:32,659 >> {'loss': 0.13052157, 'learning_rate': 4.522585e-05, 'epoch': 0.47741461623209697, 'step': 1300}\n",
            "2021-03-26 15:36:17.604651: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:36:17.604840: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:36:17 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:36:17 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:36:17 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:36:17 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:36:25,771 >> {'eval_loss': 0.10291033131735665, 'eval_precision': 0.7523191094619666, 'eval_recall': 0.7950980392156862, 'eval_f1': 0.773117254528122, 'epoch': 0.5141388174807198, 'step': 1400}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:36:25,774 >> {'loss': 0.12626323, 'learning_rate': 4.4858607e-05, 'epoch': 0.5141388174807198, 'step': 1400}\n",
            "2021-03-26 15:37:10.780353: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:37:10.780543: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:37:10 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:37:10 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:37:10 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:37:10 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:37:18,928 >> {'eval_loss': 0.10594087839126587, 'eval_precision': 0.7504504504504504, 'eval_recall': 0.8166666666666667, 'eval_f1': 0.7821596244131456, 'epoch': 0.5508630187293426, 'step': 1500}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:37:18,931 >> {'loss': 0.12339685, 'learning_rate': 4.4491368e-05, 'epoch': 0.5508630187293426, 'step': 1500}\n",
            "2021-03-26 15:38:03.864051: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:38:03.864204: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:38:03 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:38:03 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:38:03 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:38:03 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:38:11,973 >> {'eval_loss': 0.10366284847259521, 'eval_precision': 0.7766798418972332, 'eval_recall': 0.7705882352941177, 'eval_f1': 0.7736220472440944, 'epoch': 0.5875872199779655, 'step': 1600}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:38:11,976 >> {'loss': 0.12027999, 'learning_rate': 4.4124125e-05, 'epoch': 0.5875872199779655, 'step': 1600}\n",
            "2021-03-26 15:38:56.936445: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:38:56.936617: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:38:56 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:38:56 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:38:56 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:38:56 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:39:05,108 >> {'eval_loss': 0.12630640608923777, 'eval_precision': 0.6947456213511259, 'eval_recall': 0.8166666666666667, 'eval_f1': 0.7507886435331229, 'epoch': 0.6243114212265883, 'step': 1700}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:39:05,111 >> {'loss': 0.11731042, 'learning_rate': 4.3756885e-05, 'epoch': 0.6243114212265883, 'step': 1700}\n",
            "2021-03-26 15:39:50.066998: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:39:50.067175: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:39:50 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:39:50 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:39:50 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:39:50 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:39:58,232 >> {'eval_loss': 0.10186415910720825, 'eval_precision': 0.7859181731684111, 'eval_recall': 0.8098039215686275, 'eval_f1': 0.797682279092226, 'epoch': 0.6610356224752112, 'step': 1800}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:39:58,235 >> {'loss': 0.11543659, 'learning_rate': 4.3389642e-05, 'epoch': 0.6610356224752112, 'step': 1800}\n",
            "2021-03-26 15:40:43.180828: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:40:43.180993: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:40:43 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:40:43 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:40:43 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:40:43 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:40:51,331 >> {'eval_loss': 0.0980422922543117, 'eval_precision': 0.806420233463035, 'eval_recall': 0.8127450980392157, 'eval_f1': 0.8095703125, 'epoch': 0.697759823723834, 'step': 1900}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:40:51,334 >> {'loss': 0.112355754, 'learning_rate': 4.30224e-05, 'epoch': 0.697759823723834, 'step': 1900}\n",
            "2021-03-26 15:41:36.328685: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:41:36.328866: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:41:36 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:41:36 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:41:36 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:41:36 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:41:44,497 >> {'eval_loss': 0.10289431469781059, 'eval_precision': 0.7638640429338104, 'eval_recall': 0.8372549019607843, 'eval_f1': 0.7988774555659495, 'epoch': 0.7344840249724568, 'step': 2000}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:41:44,500 >> {'loss': 0.11005072, 'learning_rate': 4.2655152e-05, 'epoch': 0.7344840249724568, 'step': 2000}\n",
            "[INFO|trainer_tf.py:595] 2021-03-26 15:42:02,411 >> Saving checkpoint for step 2000 at eval/checkpoint/ckpt-2\n",
            "2021-03-26 15:42:47.658133: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:42:47.658321: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:42:47 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:42:47 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:42:47 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:42:47 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:42:55,856 >> {'eval_loss': 0.09098872116633824, 'eval_precision': 0.7908496732026143, 'eval_recall': 0.8303921568627451, 'eval_f1': 0.8101386896221904, 'epoch': 0.7712082262210797, 'step': 2100}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:42:55,859 >> {'loss': 0.10786671, 'learning_rate': 4.2287917e-05, 'epoch': 0.7712082262210797, 'step': 2100}\n",
            "2021-03-26 15:43:40.626751: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:43:40.626919: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:43:40 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:43:40 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:43:40 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:43:40 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:43:48,801 >> {'eval_loss': 0.09328849826540266, 'eval_precision': 0.7867298578199052, 'eval_recall': 0.8137254901960784, 'eval_f1': 0.7999999999999999, 'epoch': 0.8079324274697025, 'step': 2200}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:43:48,804 >> {'loss': 0.10603756, 'learning_rate': 4.1920677e-05, 'epoch': 0.8079324274697025, 'step': 2200}\n",
            "2021-03-26 15:44:33.758085: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:44:33.758256: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:44:33 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:44:33 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:44:33 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:44:33 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:44:41,912 >> {'eval_loss': 0.09817729677472796, 'eval_precision': 0.7379491673970202, 'eval_recall': 0.8254901960784313, 'eval_f1': 0.7792688570106433, 'epoch': 0.8446566287183254, 'step': 2300}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:44:41,915 >> {'loss': 0.103926264, 'learning_rate': 4.1553434e-05, 'epoch': 0.8446566287183254, 'step': 2300}\n",
            "2021-03-26 15:45:26.783016: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:45:26.783180: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:45:26 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:45:26 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:45:26 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:45:26 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:45:34,940 >> {'eval_loss': 0.09331925426210676, 'eval_precision': 0.7745283018867924, 'eval_recall': 0.8049019607843138, 'eval_f1': 0.7894230769230769, 'epoch': 0.8813808299669482, 'step': 2400}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:45:34,943 >> {'loss': 0.10196364, 'learning_rate': 4.118619e-05, 'epoch': 0.8813808299669482, 'step': 2400}\n",
            "2021-03-26 15:46:19.863439: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:46:19.863608: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:46:19 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:46:19 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:46:19 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:46:19 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:46:28,026 >> {'eval_loss': 0.10210158143724714, 'eval_precision': 0.7612903225806451, 'eval_recall': 0.8098039215686275, 'eval_f1': 0.7847980997624703, 'epoch': 0.9181050312155711, 'step': 2500}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:46:28,029 >> {'loss': 0.100186326, 'learning_rate': 4.0818948e-05, 'epoch': 0.9181050312155711, 'step': 2500}\n",
            "2021-03-26 15:47:13.003557: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:47:13.003742: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:47:13 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:47:13 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:47:13 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:47:13 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:47:21,167 >> {'eval_loss': 0.09337650878088814, 'eval_precision': 0.7958412098298677, 'eval_recall': 0.8254901960784313, 'eval_f1': 0.8103946102021174, 'epoch': 0.9548292324641939, 'step': 2600}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:47:21,170 >> {'loss': 0.09833198, 'learning_rate': 4.045171e-05, 'epoch': 0.9548292324641939, 'step': 2600}\n",
            "2021-03-26 15:48:06.127287: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:48:06.127449: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:48:06 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:48:06 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:48:06 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:48:06 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:48:14,286 >> {'eval_loss': 0.09604806559426445, 'eval_precision': 0.7681159420289855, 'eval_recall': 0.8313725490196079, 'eval_f1': 0.7984934086629002, 'epoch': 0.9915534337128168, 'step': 2700}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:48:14,289 >> {'loss': 0.09649025, 'learning_rate': 4.0084466e-05, 'epoch': 0.9915534337128168, 'step': 2700}\n",
            "2021-03-26 15:49:04.808425: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:49:04.808599: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:49:04 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:49:04 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:49:04 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:49:04 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:49:12,989 >> {'eval_loss': 0.1067480104310172, 'eval_precision': 0.7588967971530249, 'eval_recall': 0.8362745098039216, 'eval_f1': 0.7957089552238806, 'epoch': 1.0282776349614395, 'step': 2800}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:49:12,993 >> {'loss': 0.046960674, 'learning_rate': 3.9717223e-05, 'epoch': 1.0282776349614395, 'step': 2800}\n",
            "2021-03-26 15:49:57.809648: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:49:57.809818: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:49:57 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:49:57 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:49:57 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:49:57 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:50:05,976 >> {'eval_loss': 0.09790267263139997, 'eval_precision': 0.7835538752362949, 'eval_recall': 0.8127450980392157, 'eval_f1': 0.7978825794032725, 'epoch': 1.0650018362100624, 'step': 2900}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:50:05,979 >> {'loss': 0.041213382, 'learning_rate': 3.934998e-05, 'epoch': 1.0650018362100624, 'step': 2900}\n",
            "2021-03-26 15:50:50.908748: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:50:50.908916: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:50:50 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:50:50 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:50:50 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:50:50 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:50:59,048 >> {'eval_loss': 0.10123991966247559, 'eval_precision': 0.7976539589442815, 'eval_recall': 0.8, 'eval_f1': 0.7988252569750368, 'epoch': 1.1017260374586852, 'step': 3000}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:50:59,051 >> {'loss': 0.040823624, 'learning_rate': 3.8982736e-05, 'epoch': 1.1017260374586852, 'step': 3000}\n",
            "[INFO|trainer_tf.py:595] 2021-03-26 15:51:20,966 >> Saving checkpoint for step 3000 at eval/checkpoint/ckpt-3\n",
            "2021-03-26 15:52:06.200336: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:52:06.200519: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:52:06 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:52:06 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:52:06 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:52:06 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:52:14,375 >> {'eval_loss': 0.1036554319517953, 'eval_precision': 0.7783018867924528, 'eval_recall': 0.8088235294117647, 'eval_f1': 0.7932692307692307, 'epoch': 1.138450238707308, 'step': 3100}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:52:14,378 >> {'loss': 0.04053057, 'learning_rate': 3.8615497e-05, 'epoch': 1.138450238707308, 'step': 3100}\n",
            "2021-03-26 15:52:59.124878: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:52:59.125040: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:52:59 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:52:59 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:52:59 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:52:59 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:53:07,290 >> {'eval_loss': 0.09965745040348598, 'eval_precision': 0.7762430939226519, 'eval_recall': 0.8264705882352941, 'eval_f1': 0.8005698005698005, 'epoch': 1.175174439955931, 'step': 3200}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:53:07,293 >> {'loss': 0.040233273, 'learning_rate': 3.8248254e-05, 'epoch': 1.175174439955931, 'step': 3200}\n",
            "2021-03-26 15:53:52.270045: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:53:52.270217: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:53:52 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:53:52 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:53:52 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:53:52 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:54:00,396 >> {'eval_loss': 0.10486142975943429, 'eval_precision': 0.7550839964633068, 'eval_recall': 0.8372549019607843, 'eval_f1': 0.794049279404928, 'epoch': 1.2118986412045538, 'step': 3300}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:54:00,400 >> {'loss': 0.040597565, 'learning_rate': 3.7881015e-05, 'epoch': 1.2118986412045538, 'step': 3300}\n",
            "2021-03-26 15:54:45.270885: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:54:45.271043: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:54:45 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:54:45 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:54:45 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:54:45 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:54:53,434 >> {'eval_loss': 0.10044516835893903, 'eval_precision': 0.7761053621825024, 'eval_recall': 0.8088235294117647, 'eval_f1': 0.7921267402784447, 'epoch': 1.2486228424531767, 'step': 3400}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:54:53,437 >> {'loss': 0.039851885, 'learning_rate': 3.7513768e-05, 'epoch': 1.2486228424531767, 'step': 3400}\n",
            "2021-03-26 15:55:38.313685: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:55:38.313879: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:55:38 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:55:38 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:55:38 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:55:38 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:55:46,508 >> {'eval_loss': 0.10160530464989799, 'eval_precision': 0.7557182067703568, 'eval_recall': 0.8098039215686275, 'eval_f1': 0.7818267865593942, 'epoch': 1.2853470437017995, 'step': 3500}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:55:46,512 >> {'loss': 0.039637804, 'learning_rate': 3.7146525e-05, 'epoch': 1.2853470437017995, 'step': 3500}\n",
            "2021-03-26 15:56:31.366489: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:56:31.366684: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:56:31 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:56:31 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:56:31 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:56:31 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:56:39,520 >> {'eval_loss': 0.09505671262741089, 'eval_precision': 0.7883349012229539, 'eval_recall': 0.8215686274509804, 'eval_f1': 0.8046087373979837, 'epoch': 1.3220712449504224, 'step': 3600}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:56:39,523 >> {'loss': 0.040037096, 'learning_rate': 3.6779285e-05, 'epoch': 1.3220712449504224, 'step': 3600}\n",
            "2021-03-26 15:57:24.392122: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:57:24.392292: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:57:24 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:57:24 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:57:24 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:57:24 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:57:32,549 >> {'eval_loss': 0.09847309759684972, 'eval_precision': 0.7866300366300366, 'eval_recall': 0.842156862745098, 'eval_f1': 0.8134469696969697, 'epoch': 1.3587954461990452, 'step': 3700}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:57:32,552 >> {'loss': 0.039979484, 'learning_rate': 3.6412046e-05, 'epoch': 1.3587954461990452, 'step': 3700}\n",
            "2021-03-26 15:58:17.433577: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:58:17.433755: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:58:17 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:58:17 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:58:17 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:58:17 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:58:25,583 >> {'eval_loss': 0.08705099139894758, 'eval_precision': 0.7916666666666666, 'eval_recall': 0.8009803921568628, 'eval_f1': 0.7962962962962962, 'epoch': 1.395519647447668, 'step': 3800}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:58:25,586 >> {'loss': 0.039623152, 'learning_rate': 3.6044803e-05, 'epoch': 1.395519647447668, 'step': 3800}\n",
            "2021-03-26 15:59:10.507794: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 15:59:10.507972: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 15:59:10 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 15:59:10 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 15:59:10 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 15:59:10 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:59:18,677 >> {'eval_loss': 0.1027751054082598, 'eval_precision': 0.8243801652892562, 'eval_recall': 0.7823529411764706, 'eval_f1': 0.8028169014084506, 'epoch': 1.4322438486962907, 'step': 3900}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 15:59:18,680 >> {'loss': 0.03979922, 'learning_rate': 3.567756e-05, 'epoch': 1.4322438486962907, 'step': 3900}\n",
            "2021-03-26 16:00:03.544985: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 16:00:03.545151: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 16:00:03 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 16:00:03 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 16:00:03 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 16:00:03 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:00:11,696 >> {'eval_loss': 0.09832752602440971, 'eval_precision': 0.7730232558139535, 'eval_recall': 0.8147058823529412, 'eval_f1': 0.7933174224343674, 'epoch': 1.4689680499449138, 'step': 4000}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:00:11,700 >> {'loss': 0.03867416, 'learning_rate': 3.5310317e-05, 'epoch': 1.4689680499449138, 'step': 4000}\n",
            "[INFO|trainer_tf.py:595] 2021-03-26 16:00:29,906 >> Saving checkpoint for step 4000 at eval/checkpoint/ckpt-4\n",
            "2021-03-26 16:01:15.121482: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 16:01:15.121667: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 16:01:15 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 16:01:15 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 16:01:15 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 16:01:15 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:01:23,302 >> {'eval_loss': 0.09765904290335518, 'eval_precision': 0.7973609802073516, 'eval_recall': 0.8294117647058824, 'eval_f1': 0.8130706391158097, 'epoch': 1.5056922511935364, 'step': 4100}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:01:23,306 >> {'loss': 0.03830278, 'learning_rate': 3.4943074e-05, 'epoch': 1.5056922511935364, 'step': 4100}\n",
            "2021-03-26 16:02:08.006236: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 16:02:08.006402: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 16:02:08 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 16:02:08 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 16:02:08 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 16:02:08 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:02:16,157 >> {'eval_loss': 0.09996876546314784, 'eval_precision': 0.7961538461538461, 'eval_recall': 0.8117647058823529, 'eval_f1': 0.803883495145631, 'epoch': 1.5424164524421595, 'step': 4200}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:02:16,161 >> {'loss': 0.037379418, 'learning_rate': 3.4575834e-05, 'epoch': 1.5424164524421595, 'step': 4200}\n",
            "2021-03-26 16:03:01.070944: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 16:03:01.071113: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 16:03:01 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 16:03:01 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 16:03:01 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 16:03:01 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:03:09,218 >> {'eval_loss': 0.10170501470565796, 'eval_precision': 0.8190376569037657, 'eval_recall': 0.7676470588235295, 'eval_f1': 0.7925101214574899, 'epoch': 1.5791406536907822, 'step': 4300}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:03:09,222 >> {'loss': 0.037306134, 'learning_rate': 3.420859e-05, 'epoch': 1.5791406536907822, 'step': 4300}\n",
            "2021-03-26 16:03:54.036161: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 16:03:54.036321: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 16:03:54 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 16:03:54 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 16:03:54 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 16:03:54 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:04:02,196 >> {'eval_loss': 0.1038458091872079, 'eval_precision': 0.7587758775877588, 'eval_recall': 0.8264705882352941, 'eval_f1': 0.7911778507742844, 'epoch': 1.615864854939405, 'step': 4400}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:04:02,199 >> {'loss': 0.037055727, 'learning_rate': 3.3841352e-05, 'epoch': 1.615864854939405, 'step': 4400}\n",
            "2021-03-26 16:04:47.100713: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 16:04:47.100889: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 16:04:47 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 16:04:47 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 16:04:47 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 16:04:47 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:04:55,275 >> {'eval_loss': 0.10366325719015938, 'eval_precision': 0.7751865671641791, 'eval_recall': 0.8147058823529412, 'eval_f1': 0.7944550669216062, 'epoch': 1.6525890561880279, 'step': 4500}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:04:55,278 >> {'loss': 0.0365602, 'learning_rate': 3.347411e-05, 'epoch': 1.6525890561880279, 'step': 4500}\n",
            "2021-03-26 16:05:40.176075: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 16:05:40.176240: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 16:05:40 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 16:05:40 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 16:05:40 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 16:05:40 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:05:48,342 >> {'eval_loss': 0.10664875166756767, 'eval_precision': 0.7960151802656547, 'eval_recall': 0.8225490196078431, 'eval_f1': 0.8090646094503374, 'epoch': 1.6893132574366507, 'step': 4600}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:05:48,346 >> {'loss': 0.03615187, 'learning_rate': 3.3106866e-05, 'epoch': 1.6893132574366507, 'step': 4600}\n",
            "2021-03-26 16:06:33.230446: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 16:06:33.230626: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 16:06:33 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 16:06:33 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 16:06:33 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 16:06:33 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:06:41,394 >> {'eval_loss': 0.10427339587892805, 'eval_precision': 0.8045409674234946, 'eval_recall': 0.7990196078431373, 'eval_f1': 0.8017707820954255, 'epoch': 1.7260374586852736, 'step': 4700}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:06:41,397 >> {'loss': 0.036021322, 'learning_rate': 3.2739626e-05, 'epoch': 1.7260374586852736, 'step': 4700}\n",
            "2021-03-26 16:07:26.318823: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 16:07:26.318988: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 16:07:26 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 16:07:26 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 16:07:26 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 16:07:26 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:07:34,498 >> {'eval_loss': 0.10517762388501849, 'eval_precision': 0.7923292797006548, 'eval_recall': 0.8303921568627451, 'eval_f1': 0.8109143130684539, 'epoch': 1.7627616599338964, 'step': 4800}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:07:34,501 >> {'loss': 0.035375208, 'learning_rate': 3.2372383e-05, 'epoch': 1.7627616599338964, 'step': 4800}\n",
            "2021-03-26 16:08:19.376749: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 16:08:19.376919: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 16:08:19 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 16:08:19 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 16:08:19 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 16:08:19 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:08:27,521 >> {'eval_loss': 0.10049498932702201, 'eval_precision': 0.7832558139534884, 'eval_recall': 0.8254901960784313, 'eval_f1': 0.8038186157517899, 'epoch': 1.7994858611825193, 'step': 4900}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:08:27,524 >> {'loss': 0.03509914, 'learning_rate': 3.2005144e-05, 'epoch': 1.7994858611825193, 'step': 4900}\n",
            "2021-03-26 16:09:12.422584: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 16:09:12.422799: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 16:09:12 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 16:09:12 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 16:09:12 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 16:09:12 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:09:20,540 >> {'eval_loss': 0.1111534493310111, 'eval_precision': 0.7542754275427542, 'eval_recall': 0.8215686274509804, 'eval_f1': 0.7864852182074143, 'epoch': 1.8362100624311422, 'step': 5000}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:09:20,543 >> {'loss': 0.034813367, 'learning_rate': 3.16379e-05, 'epoch': 1.8362100624311422, 'step': 5000}\n",
            "[INFO|trainer_tf.py:595] 2021-03-26 16:09:38,086 >> Saving checkpoint for step 5000 at eval/checkpoint/ckpt-5\n",
            "2021-03-26 16:10:23.365020: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 16:10:23.365187: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 16:10:23 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 16:10:23 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 16:10:23 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 16:10:23 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:10:31,561 >> {'eval_loss': 0.10004328829901558, 'eval_precision': 0.7784810126582279, 'eval_recall': 0.8441176470588235, 'eval_f1': 0.8099717779868298, 'epoch': 1.872934263679765, 'step': 5100}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:10:31,564 >> {'loss': 0.034452263, 'learning_rate': 3.1270658e-05, 'epoch': 1.872934263679765, 'step': 5100}\n",
            "2021-03-26 16:11:16.288259: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 16:11:16.288434: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 16:11:16 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 16:11:16 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 16:11:16 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 16:11:16 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:11:24,446 >> {'eval_loss': 0.10808110237121582, 'eval_precision': 0.7680365296803653, 'eval_recall': 0.8245098039215686, 'eval_f1': 0.7952718676122932, 'epoch': 1.9096584649283876, 'step': 5200}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:11:24,450 >> {'loss': 0.034339946, 'learning_rate': 3.0903415e-05, 'epoch': 1.9096584649283876, 'step': 5200}\n",
            "2021-03-26 16:12:09.432264: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 16:12:09.432463: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 16:12:09 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 16:12:09 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 16:12:09 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 16:12:09 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:12:17,589 >> {'eval_loss': 0.10168365921292986, 'eval_precision': 0.7800751879699248, 'eval_recall': 0.8137254901960784, 'eval_f1': 0.7965451055662187, 'epoch': 1.9463826661770107, 'step': 5300}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:12:17,592 >> {'loss': 0.034284346, 'learning_rate': 3.0536175e-05, 'epoch': 1.9463826661770107, 'step': 5300}\n",
            "2021-03-26 16:13:02.436554: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 16:13:02.436735: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 16:13:02 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 16:13:02 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 16:13:02 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 16:13:02 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:13:10,600 >> {'eval_loss': 0.1015893987246922, 'eval_precision': 0.8040345821325648, 'eval_recall': 0.8205882352941176, 'eval_f1': 0.8122270742358079, 'epoch': 1.9831068674256334, 'step': 5400}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:13:10,603 >> {'loss': 0.03401759, 'learning_rate': 3.0168929e-05, 'epoch': 1.9831068674256334, 'step': 5400}\n",
            "2021-03-26 16:14:01.072367: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 16:14:01.072531: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 16:14:01 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 16:14:01 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 16:14:01 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 16:14:01 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:14:09,290 >> {'eval_loss': 0.10256563765662056, 'eval_precision': 0.7863805970149254, 'eval_recall': 0.8264705882352941, 'eval_f1': 0.8059273422562141, 'epoch': 2.0198310686742564, 'step': 5500}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:14:09,293 >> {'loss': 0.023309318, 'learning_rate': 2.980169e-05, 'epoch': 2.0198310686742564, 'step': 5500}\n",
            "2021-03-26 16:14:54.136115: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 16:14:54.136286: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 16:14:54 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 16:14:54 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 16:14:54 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 16:14:54 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:15:02,296 >> {'eval_loss': 0.10697038684572492, 'eval_precision': 0.7769516728624535, 'eval_recall': 0.8196078431372549, 'eval_f1': 0.7977099236641222, 'epoch': 2.056555269922879, 'step': 5600}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:15:02,299 >> {'loss': 0.023874609, 'learning_rate': 2.9434448e-05, 'epoch': 2.056555269922879, 'step': 5600}\n",
            "2021-03-26 16:15:47.251560: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 16:15:47.251734: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 16:15:47 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 16:15:47 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 16:15:47 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 16:15:47 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:15:55,407 >> {'eval_loss': 0.10339888504573277, 'eval_precision': 0.7867165575304023, 'eval_recall': 0.8245098039215686, 'eval_f1': 0.8051699377692676, 'epoch': 2.093279471171502, 'step': 5700}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:15:55,410 >> {'loss': 0.021295486, 'learning_rate': 2.9067203e-05, 'epoch': 2.093279471171502, 'step': 5700}\n",
            "2021-03-26 16:16:40.319616: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 16:16:40.319830: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 16:16:40 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 16:16:40 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 16:16:40 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 16:16:40 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:16:48,483 >> {'eval_loss': 0.10714757442474365, 'eval_precision': 0.7745716862037872, 'eval_recall': 0.842156862745098, 'eval_f1': 0.8069516204790983, 'epoch': 2.130003672420125, 'step': 5800}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:16:48,486 >> {'loss': 0.020168643, 'learning_rate': 2.8699964e-05, 'epoch': 2.130003672420125, 'step': 5800}\n",
            "2021-03-26 16:17:33.288158: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 16:17:33.288323: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 16:17:33 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 16:17:33 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 16:17:33 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 16:17:33 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:17:41,434 >> {'eval_loss': 0.10219779184886388, 'eval_precision': 0.8086710650329878, 'eval_recall': 0.8411764705882353, 'eval_f1': 0.8246035559827006, 'epoch': 2.166727873668748, 'step': 5900}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:17:41,438 >> {'loss': 0.018969826, 'learning_rate': 2.833272e-05, 'epoch': 2.166727873668748, 'step': 5900}\n",
            "2021-03-26 16:18:26.301944: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 16:18:26.302127: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 16:18:26 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 16:18:26 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 16:18:26 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 16:18:26 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:18:34,452 >> {'eval_loss': 0.11134306873594012, 'eval_precision': 0.7659192825112108, 'eval_recall': 0.8372549019607843, 'eval_f1': 0.8, 'epoch': 2.2034520749173705, 'step': 6000}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:18:34,455 >> {'loss': 0.018898588, 'learning_rate': 2.7965476e-05, 'epoch': 2.2034520749173705, 'step': 6000}\n",
            "[INFO|trainer_tf.py:595] 2021-03-26 16:18:56,812 >> Saving checkpoint for step 6000 at eval/checkpoint/ckpt-6\n",
            "2021-03-26 16:19:41.996369: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 16:19:41.996536: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 16:19:41 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 16:19:41 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 16:19:41 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 16:19:41 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:19:50,163 >> {'eval_loss': 0.09780676024300712, 'eval_precision': 0.8166332665330661, 'eval_recall': 0.7990196078431373, 'eval_f1': 0.8077304261645195, 'epoch': 2.2401762761659936, 'step': 6100}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:19:50,166 >> {'loss': 0.01880807, 'learning_rate': 2.7598237e-05, 'epoch': 2.2401762761659936, 'step': 6100}\n",
            "2021-03-26 16:20:34.884849: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 16:20:34.885030: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 16:20:34 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 16:20:34 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 16:20:34 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 16:20:34 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:20:43,044 >> {'eval_loss': 0.10921078068869454, 'eval_precision': 0.8001939864209505, 'eval_recall': 0.8088235294117647, 'eval_f1': 0.8044856167723061, 'epoch': 2.276900477414616, 'step': 6200}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:20:43,047 >> {'loss': 0.019421052, 'learning_rate': 2.7230995e-05, 'epoch': 2.276900477414616, 'step': 6200}\n",
            "2021-03-26 16:21:28.020823: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 16:21:28.020995: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 16:21:28 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 16:21:28 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 16:21:28 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 16:21:28 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:21:36,156 >> {'eval_loss': 0.10705946172986712, 'eval_precision': 0.7381567614125754, 'eval_recall': 0.8401960784313726, 'eval_f1': 0.7858780375974325, 'epoch': 2.3136246786632393, 'step': 6300}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:21:36,160 >> {'loss': 0.0195239, 'learning_rate': 2.6863752e-05, 'epoch': 2.3136246786632393, 'step': 6300}\n",
            "2021-03-26 16:22:21.026869: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 16:22:21.027027: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 16:22:21 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 16:22:21 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 16:22:21 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 16:22:21 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:22:29,192 >> {'eval_loss': 0.10490106684820992, 'eval_precision': 0.7933333333333333, 'eval_recall': 0.8166666666666667, 'eval_f1': 0.8048309178743962, 'epoch': 2.350348879911862, 'step': 6400}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:22:29,195 >> {'loss': 0.019408543, 'learning_rate': 2.6496511e-05, 'epoch': 2.350348879911862, 'step': 6400}\n",
            "2021-03-26 16:23:14.102264: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 16:23:14.102433: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 16:23:14 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 16:23:14 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 16:23:14 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 16:23:14 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:23:22,255 >> {'eval_loss': 0.10301383904048375, 'eval_precision': 0.7844423617619494, 'eval_recall': 0.8205882352941176, 'eval_f1': 0.8021082894106373, 'epoch': 2.3870730811604846, 'step': 6500}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:23:22,258 >> {'loss': 0.020096393, 'learning_rate': 2.612927e-05, 'epoch': 2.3870730811604846, 'step': 6500}\n",
            "2021-03-26 16:24:07.110843: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 16:24:07.111006: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 16:24:07 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 16:24:07 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 16:24:07 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 16:24:07 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:24:15,259 >> {'eval_loss': 0.106013970715659, 'eval_precision': 0.8104448742746615, 'eval_recall': 0.8215686274509804, 'eval_f1': 0.8159688412852969, 'epoch': 2.4237972824091076, 'step': 6600}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:24:15,262 >> {'loss': 0.019506091, 'learning_rate': 2.5762027e-05, 'epoch': 2.4237972824091076, 'step': 6600}\n",
            "2021-03-26 16:25:00.103193: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 16:25:00.103360: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 16:25:00 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 16:25:00 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 16:25:00 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 16:25:00 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:25:08,264 >> {'eval_loss': 0.10403740406036377, 'eval_precision': 0.7608503100088574, 'eval_recall': 0.842156862745098, 'eval_f1': 0.7994416007445324, 'epoch': 2.4605214836577303, 'step': 6700}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:25:08,267 >> {'loss': 0.01916925, 'learning_rate': 2.5394786e-05, 'epoch': 2.4605214836577303, 'step': 6700}\n",
            "2021-03-26 16:25:53.135996: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-03-26 16:25:53.136161: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_8753\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "03/26/2021 16:25:53 - INFO - utils_ner -   ***** Running Evaluation *****\n",
            "03/26/2021 16:25:53 - INFO - utils_ner -     Num examples in dataset = 446\n",
            "03/26/2021 16:25:53 - INFO - utils_ner -     Num examples in used in evaluation = 448\n",
            "03/26/2021 16:25:53 - INFO - utils_ner -     Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:26:01,299 >> {'eval_loss': 0.10085488217217582, 'eval_precision': 0.7895716945996276, 'eval_recall': 0.8313725490196079, 'eval_f1': 0.8099331423113657, 'epoch': 2.4972456849063533, 'step': 6800}\n",
            "[INFO|trainer_tf.py:404] 2021-03-26 16:26:01,302 >> {'loss': 0.019283086, 'learning_rate': 2.5027543e-05, 'epoch': 2.4972456849063533, 'step': 6800}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6XQQZaRkPYu"
      },
      "source": [
        "!pip install -q transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIqsqGnYHQI3"
      },
      "source": [
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForPreTraining\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = TFBertForPreTraining.from_pretrained('bert-base-uncased')\n",
        "input_ids = tf.constant(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True))[None, :]  # Batch size 1\n",
        "outputs = model(input_ids)\n",
        "prediction_scores, seq_relationship_scores = outputs[:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctkfSnOdkGyd"
      },
      "source": [
        "getattr(model, 'bert')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ksz-_fngke-R"
      },
      "source": [
        "from transformers import TFBertForTokenClassification\n",
        "model = TFBertForTokenClassification.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRhzd7H2f2Ih"
      },
      "source": [
        "model.layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_ACLQQegJx9"
      },
      "source": [
        "model.layers[2].__dict__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXwPixA-aAvN"
      },
      "source": [
        "getattr(model, 'bert').count_params()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ci7BpG-pjRxH"
      },
      "source": [
        "model.layers[2].count_params()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyd4RiC6hf-e"
      },
      "source": [
        "from keras.utils.vis_utils import plot_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAaAfwbwhj6A"
      },
      "source": [
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fnc37IAdf4q2"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQiRQnIwabIc"
      },
      "source": [
        "model.layers[0]._layers[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX8534Jdf611"
      },
      "source": [
        "model.layers[0]._layers[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SH9e5Ia1fhLr"
      },
      "source": [
        "model.layers[0]._layers[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQWjnx9dfiMf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}