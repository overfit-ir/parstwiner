{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tempo.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/overfit-ir/persian-twitter-ner/blob/master/fine-tune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebZsy7HYObGg"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF-7m57sBgdp"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1Awb1uBEExX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc59049b-279a-4a3f-cb9b-3c022b37fb59"
      },
      "source": [
        "!wget -q --show-progress 'https://raw.githubusercontent.com/overfit-ir/persian-twitter-ner/master/twitter_data/persian-ner-twitter-data/train.txt'\n",
        "!wget -q --show-progress 'https://raw.githubusercontent.com/overfit-ir/persian-twitter-ner/master/twitter_data/persian-ner-twitter-data/test.txt'\n",
        "!wget -q --show-progress 'https://raw.githubusercontent.com/overfit-ir/persian-twitter-ner/master/twitter_data/persian-ner-twitter-data/dev.txt'\n",
        "!mkdir data && mv train.txt data && mv test.txt data && mv dev.txt data"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train.txt           100%[===================>]   2.20M  --.-KB/s    in 0.04s   \n",
            "test.txt            100%[===================>] 108.10K  --.-KB/s    in 0.003s  \n",
            "dev.txt             100%[===================>] 160.83K  --.-KB/s    in 0.003s  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuuoB7sxjk0Y"
      },
      "source": [
        "# Benchmark2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0dGPIj5MTqW"
      },
      "source": [
        "!cat data/train.txt | grep -v \"^#\" | cut -f 1,2 | tr '\\t' ' ' > train.txt.tmp\n",
        "!cat data/test.txt | grep -v \"^#\" | cut -f 1,2 | tr '\\t' ' ' > test.txt.tmp\n",
        "!cat data/dev.txt | grep -v \"^#\" | cut -f 1,2 | tr '\\t' ' ' > dev.txt.tmp"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ui3HZfRwfwTH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d03bcb19-42a5-4a9b-9c54-695d4f15a0ce"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 83, done.\u001b[K\n",
            "remote: Counting objects: 100% (83/83), done.\u001b[K\n",
            "remote: Compressing objects: 100% (67/67), done.\u001b[K\n",
            "remote: Total 62958 (delta 26), reused 60 (delta 14), pack-reused 62875\u001b[K\n",
            "Receiving objects: 100% (62958/62958), 48.07 MiB | 28.94 MiB/s, done.\n",
            "Resolving deltas: 100% (44579/44579), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZHBg4CauUCM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3631736-47cf-480c-dd7d-dead8af0d25c"
      },
      "source": [
        "!pip -q install transformers/\n",
        "!pip -q install sentencepiece"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 22.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 51.8MB/s \n",
            "\u001b[?25h  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 16.0MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Atfg8k8Ajqv-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3d0e31e-4de5-4968-843d-c64f1ca0aedb"
      },
      "source": [
        "!wget \"https://raw.githubusercontent.com/stefan-it/fine-tuned-berts-seq/master/scripts/preprocess.py\""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-20 03:53:57--  https://raw.githubusercontent.com/stefan-it/fine-tuned-berts-seq/master/scripts/preprocess.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 991 [text/plain]\n",
            "Saving to: ‘preprocess.py’\n",
            "\n",
            "\rpreprocess.py         0%[                    ]       0  --.-KB/s               \rpreprocess.py       100%[===================>]     991  --.-KB/s    in 0s      \n",
            "\n",
            "2021-02-20 03:53:57 (59.3 MB/s) - ‘preprocess.py’ saved [991/991]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itWy78SnkBlk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4364c64f-3fbc-4468-faee-3ff347f2513a"
      },
      "source": [
        "!python3 preprocess.py dev.txt.tmp HooshvareLab/bert-base-parsbert-uncased 256 > dev.txt\n",
        "!python3 preprocess.py train.txt.tmp HooshvareLab/bert-base-parsbert-uncased 256 > train.txt\n",
        "!python3 preprocess.py test.txt.tmp HooshvareLab/bert-base-parsbert-uncased 256 > test.txt"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rDownloading:   0% 0.00/434 [00:00<?, ?B/s]\rDownloading: 100% 434/434 [00:00<00:00, 638kB/s]\n",
            "\rDownloading:   0% 0.00/1.22M [00:00<?, ?B/s]\rDownloading: 100% 1.22M/1.22M [00:00<00:00, 36.2MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdHj8GTrlBnr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "573634a4-8eeb-4c3c-e181-2acf5aa680cd"
      },
      "source": [
        "!pip -q install -r transformers/examples/token-classification/requirements.txt"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 51kB 7.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 184kB 27.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 14.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 245kB 56.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 20.7MB 1.5MB/s \n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdElNDJWOZGe"
      },
      "source": [
        "!pip -q install conllu"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgZbOD--k4LN"
      },
      "source": [
        "!mkdir processed_data\n",
        "!mv train.txt processed_data/ && mv test.txt processed_data/ && mv dev.txt processed_data/"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iw_LDa1JpOcf"
      },
      "source": [
        "!cat processed_data/train.txt processed_data/test.txt processed_data/dev.txt | cut -d \" \" -f 2 | grep -v \"^$\"| sort | uniq > labels.txt"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTDxhn3WOCiT",
        "outputId": "45cea018-2fca-4312-ea98-8b503bb4b864",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python3 transformers/examples/legacy/token-classification/run_tf_ner.py --help"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-20 03:57:02.220360: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "usage: run_tf_ner.py [-h] --model_name_or_path MODEL_NAME_OR_PATH\n",
            "                     [--config_name CONFIG_NAME] [--task_type TASK_TYPE]\n",
            "                     [--tokenizer_name TOKENIZER_NAME] [--use_fast [USE_FAST]]\n",
            "                     [--cache_dir CACHE_DIR] --data_dir DATA_DIR --labels\n",
            "                     LABELS [--max_seq_length MAX_SEQ_LENGTH]\n",
            "                     [--overwrite_cache [OVERWRITE_CACHE]]\n",
            "                     [--output_dir OUTPUT_DIR]\n",
            "                     [--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]\n",
            "                     [--do_train [DO_TRAIN]] [--do_eval [DO_EVAL]]\n",
            "                     [--do_predict [DO_PREDICT]]\n",
            "                     [--evaluation_strategy {no,steps,epoch}]\n",
            "                     [--prediction_loss_only [PREDICTION_LOSS_ONLY]]\n",
            "                     [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n",
            "                     [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\n",
            "                     [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n",
            "                     [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
            "                     [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
            "                     [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]\n",
            "                     [--learning_rate LEARNING_RATE]\n",
            "                     [--weight_decay WEIGHT_DECAY] [--adam_beta1 ADAM_BETA1]\n",
            "                     [--adam_beta2 ADAM_BETA2] [--adam_epsilon ADAM_EPSILON]\n",
            "                     [--max_grad_norm MAX_GRAD_NORM]\n",
            "                     [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
            "                     [--max_steps MAX_STEPS]\n",
            "                     [--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}]\n",
            "                     [--warmup_ratio WARMUP_RATIO]\n",
            "                     [--warmup_steps WARMUP_STEPS] [--logging_dir LOGGING_DIR]\n",
            "                     [--logging_strategy {no,steps,epoch}]\n",
            "                     [--logging_first_step [LOGGING_FIRST_STEP]]\n",
            "                     [--logging_steps LOGGING_STEPS] [--save_steps SAVE_STEPS]\n",
            "                     [--save_total_limit SAVE_TOTAL_LIMIT]\n",
            "                     [--no_cuda [NO_CUDA]] [--seed SEED] [--fp16 [FP16]]\n",
            "                     [--fp16_opt_level FP16_OPT_LEVEL]\n",
            "                     [--fp16_backend {auto,amp,apex}]\n",
            "                     [--fp16_full_eval [FP16_FULL_EVAL]]\n",
            "                     [--local_rank LOCAL_RANK] [--tpu_num_cores TPU_NUM_CORES]\n",
            "                     [--tpu_metrics_debug [TPU_METRICS_DEBUG]]\n",
            "                     [--debug [DEBUG]]\n",
            "                     [--dataloader_drop_last [DATALOADER_DROP_LAST]]\n",
            "                     [--eval_steps EVAL_STEPS]\n",
            "                     [--dataloader_num_workers DATALOADER_NUM_WORKERS]\n",
            "                     [--past_index PAST_INDEX] [--run_name RUN_NAME]\n",
            "                     [--disable_tqdm DISABLE_TQDM]\n",
            "                     [--no_remove_unused_columns]\n",
            "                     [--remove_unused_columns [REMOVE_UNUSED_COLUMNS]]\n",
            "                     [--label_names LABEL_NAMES [LABEL_NAMES ...]]\n",
            "                     [--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]\n",
            "                     [--metric_for_best_model METRIC_FOR_BEST_MODEL]\n",
            "                     [--greater_is_better GREATER_IS_BETTER]\n",
            "                     [--ignore_data_skip [IGNORE_DATA_SKIP]]\n",
            "                     [--sharded_ddp [SHARDED_DDP]] [--deepspeed DEEPSPEED]\n",
            "                     [--label_smoothing_factor LABEL_SMOOTHING_FACTOR]\n",
            "                     [--adafactor [ADAFACTOR]]\n",
            "                     [--group_by_length [GROUP_BY_LENGTH]]\n",
            "                     [--report_to REPORT_TO [REPORT_TO ...]]\n",
            "                     [--ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS]\n",
            "                     [--no_dataloader_pin_memory]\n",
            "                     [--dataloader_pin_memory [DATALOADER_PIN_MEMORY]]\n",
            "                     [--skip_memory_metrics [SKIP_MEMORY_METRICS]]\n",
            "                     [--tpu_name TPU_NAME] [--tpu_zone TPU_ZONE]\n",
            "                     [--gcp_project GCP_PROJECT] [--poly_power POLY_POWER]\n",
            "                     [--xla [XLA]]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --model_name_or_path MODEL_NAME_OR_PATH\n",
            "                        Path to pretrained model or model identifier from\n",
            "                        huggingface.co/models\n",
            "  --config_name CONFIG_NAME\n",
            "                        Pretrained config name or path if not the same as\n",
            "                        model_name\n",
            "  --task_type TASK_TYPE\n",
            "                        Task type to fine tune in training (e.g. NER, POS,\n",
            "                        etc)\n",
            "  --tokenizer_name TOKENIZER_NAME\n",
            "                        Pretrained tokenizer name or path if not the same as\n",
            "                        model_name\n",
            "  --use_fast [USE_FAST]\n",
            "                        Set this flag to use fast tokenization.\n",
            "  --cache_dir CACHE_DIR\n",
            "                        Where do you want to store the pretrained models\n",
            "                        downloaded from huggingface.co\n",
            "  --data_dir DATA_DIR   The input data dir. Should contain the .txt files for\n",
            "                        a CoNLL-2003-formatted task.\n",
            "  --labels LABELS       Path to a file containing all labels. If not\n",
            "                        specified, CoNLL-2003 labels are used.\n",
            "  --max_seq_length MAX_SEQ_LENGTH\n",
            "                        The maximum total input sequence length after\n",
            "                        tokenization. Sequences longer than this will be\n",
            "                        truncated, sequences shorter will be padded.\n",
            "  --overwrite_cache [OVERWRITE_CACHE]\n",
            "                        Overwrite the cached training and evaluation sets\n",
            "  --output_dir OUTPUT_DIR\n",
            "                        The output directory where the model predictions and\n",
            "                        checkpoints will be written.\n",
            "  --overwrite_output_dir [OVERWRITE_OUTPUT_DIR]\n",
            "                        Overwrite the content of the output directory.Use this\n",
            "                        to continue training if output_dir points to a\n",
            "                        checkpoint directory.\n",
            "  --do_train [DO_TRAIN]\n",
            "                        Whether to run training.\n",
            "  --do_eval [DO_EVAL]   Whether to run eval on the dev set.\n",
            "  --do_predict [DO_PREDICT]\n",
            "                        Whether to run predictions on the test set.\n",
            "  --evaluation_strategy {no,steps,epoch}\n",
            "                        The evaluation strategy to use.\n",
            "  --prediction_loss_only [PREDICTION_LOSS_ONLY]\n",
            "                        When performing evaluation and predictions, only\n",
            "                        returns the loss.\n",
            "  --per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE\n",
            "                        Batch size per GPU/TPU core/CPU for training.\n",
            "  --per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE\n",
            "                        Batch size per GPU/TPU core/CPU for evaluation.\n",
            "  --per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE\n",
            "                        Deprecated, the use of `--per_device_train_batch_size`\n",
            "                        is preferred. Batch size per GPU/TPU core/CPU for\n",
            "                        training.\n",
            "  --per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE\n",
            "                        Deprecated, the use of `--per_device_eval_batch_size`\n",
            "                        is preferred.Batch size per GPU/TPU core/CPU for\n",
            "                        evaluation.\n",
            "  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS\n",
            "                        Number of updates steps to accumulate before\n",
            "                        performing a backward/update pass.\n",
            "  --eval_accumulation_steps EVAL_ACCUMULATION_STEPS\n",
            "                        Number of predictions steps to accumulate before\n",
            "                        moving the tensors to the CPU.\n",
            "  --learning_rate LEARNING_RATE\n",
            "                        The initial learning rate for AdamW.\n",
            "  --weight_decay WEIGHT_DECAY\n",
            "                        Weight decay for AdamW if we apply some.\n",
            "  --adam_beta1 ADAM_BETA1\n",
            "                        Beta1 for AdamW optimizer\n",
            "  --adam_beta2 ADAM_BETA2\n",
            "                        Beta2 for AdamW optimizer\n",
            "  --adam_epsilon ADAM_EPSILON\n",
            "                        Epsilon for AdamW optimizer.\n",
            "  --max_grad_norm MAX_GRAD_NORM\n",
            "                        Max gradient norm.\n",
            "  --num_train_epochs NUM_TRAIN_EPOCHS\n",
            "                        Total number of training epochs to perform.\n",
            "  --max_steps MAX_STEPS\n",
            "                        If > 0: set total number of training steps to perform.\n",
            "                        Override num_train_epochs.\n",
            "  --lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}\n",
            "                        The scheduler type to use.\n",
            "  --warmup_ratio WARMUP_RATIO\n",
            "                        Linear warmup over warmup_ratio fraction of total\n",
            "                        steps.\n",
            "  --warmup_steps WARMUP_STEPS\n",
            "                        Linear warmup over warmup_steps.\n",
            "  --logging_dir LOGGING_DIR\n",
            "                        Tensorboard log dir.\n",
            "  --logging_strategy {no,steps,epoch}\n",
            "                        The logging strategy to use.\n",
            "  --logging_first_step [LOGGING_FIRST_STEP]\n",
            "                        Log the first global_step\n",
            "  --logging_steps LOGGING_STEPS\n",
            "                        Log every X updates steps.\n",
            "  --save_steps SAVE_STEPS\n",
            "                        Save checkpoint every X updates steps.\n",
            "  --save_total_limit SAVE_TOTAL_LIMIT\n",
            "                        Limit the total amount of checkpoints.Deletes the\n",
            "                        older checkpoints in the output_dir. Default is\n",
            "                        unlimited checkpoints\n",
            "  --no_cuda [NO_CUDA]   Do not use CUDA even when it is available\n",
            "  --seed SEED           Random seed that will be set at the beginning of\n",
            "                        training.\n",
            "  --fp16 [FP16]         Whether to use 16-bit (mixed) precision instead of\n",
            "                        32-bit\n",
            "  --fp16_opt_level FP16_OPT_LEVEL\n",
            "                        For fp16: Apex AMP optimization level selected in\n",
            "                        ['O0', 'O1', 'O2', and 'O3'].See details at\n",
            "                        https://nvidia.github.io/apex/amp.html\n",
            "  --fp16_backend {auto,amp,apex}\n",
            "                        The backend to be used for mixed precision.\n",
            "  --fp16_full_eval [FP16_FULL_EVAL]\n",
            "                        Whether to use full 16-bit precision evaluation\n",
            "                        instead of 32-bit\n",
            "  --local_rank LOCAL_RANK\n",
            "                        For distributed training: local_rank\n",
            "  --tpu_num_cores TPU_NUM_CORES\n",
            "                        TPU: Number of TPU cores (automatically passed by\n",
            "                        launcher script)\n",
            "  --tpu_metrics_debug [TPU_METRICS_DEBUG]\n",
            "                        Deprecated, the use of `--debug` is preferred. TPU:\n",
            "                        Whether to print debug metrics\n",
            "  --debug [DEBUG]       Whether to print debug metrics on TPU\n",
            "  --dataloader_drop_last [DATALOADER_DROP_LAST]\n",
            "                        Drop the last incomplete batch if it is not divisible\n",
            "                        by the batch size.\n",
            "  --eval_steps EVAL_STEPS\n",
            "                        Run an evaluation every X steps.\n",
            "  --dataloader_num_workers DATALOADER_NUM_WORKERS\n",
            "                        Number of subprocesses to use for data loading\n",
            "                        (PyTorch only). 0 means that the data will be loaded\n",
            "                        in the main process.\n",
            "  --past_index PAST_INDEX\n",
            "                        If >=0, uses the corresponding part of the output as\n",
            "                        the past state for next step.\n",
            "  --run_name RUN_NAME   An optional descriptor for the run. Notably used for\n",
            "                        wandb logging.\n",
            "  --disable_tqdm DISABLE_TQDM\n",
            "                        Whether or not to disable the tqdm progress bars.\n",
            "  --no_remove_unused_columns\n",
            "                        Remove columns not required by the model when using an\n",
            "                        nlp.Dataset.\n",
            "  --remove_unused_columns [REMOVE_UNUSED_COLUMNS]\n",
            "                        Remove columns not required by the model when using an\n",
            "                        nlp.Dataset.\n",
            "  --label_names LABEL_NAMES [LABEL_NAMES ...]\n",
            "                        The list of keys in your dictionary of inputs that\n",
            "                        correspond to the labels.\n",
            "  --load_best_model_at_end [LOAD_BEST_MODEL_AT_END]\n",
            "                        Whether or not to load the best model found during\n",
            "                        training at the end of training.\n",
            "  --metric_for_best_model METRIC_FOR_BEST_MODEL\n",
            "                        The metric to use to compare two different models.\n",
            "  --greater_is_better GREATER_IS_BETTER\n",
            "                        Whether the `metric_for_best_model` should be\n",
            "                        maximized or not.\n",
            "  --ignore_data_skip [IGNORE_DATA_SKIP]\n",
            "                        When resuming training, whether or not to skip the\n",
            "                        first epochs and batches to get to the same training\n",
            "                        data.\n",
            "  --sharded_ddp [SHARDED_DDP]\n",
            "                        Whether or not to use sharded DDP training (in\n",
            "                        distributed training only).\n",
            "  --deepspeed DEEPSPEED\n",
            "                        Enable deepspeed and pass the path to deepspeed json\n",
            "                        config file (e.g. ds_config.json)\n",
            "  --label_smoothing_factor LABEL_SMOOTHING_FACTOR\n",
            "                        The label smoothing epsilon to apply (zero means no\n",
            "                        label smoothing).\n",
            "  --adafactor [ADAFACTOR]\n",
            "                        Whether or not to replace AdamW by Adafactor.\n",
            "  --group_by_length [GROUP_BY_LENGTH]\n",
            "                        Whether or not to group samples of roughly the same\n",
            "                        length together when batching.\n",
            "  --report_to REPORT_TO [REPORT_TO ...]\n",
            "                        The list of integrations to report the results and\n",
            "                        logs to.\n",
            "  --ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS\n",
            "                        When using distributed training, the value of the flag\n",
            "                        `find_unused_parameters` passed to\n",
            "                        `DistributedDataParallel`.\n",
            "  --no_dataloader_pin_memory\n",
            "                        Whether or not to pin memory for DataLoader.\n",
            "  --dataloader_pin_memory [DATALOADER_PIN_MEMORY]\n",
            "                        Whether or not to pin memory for DataLoader.\n",
            "  --skip_memory_metrics [SKIP_MEMORY_METRICS]\n",
            "                        Whether or not to skip adding of memory profiler\n",
            "                        reports to metrics.\n",
            "  --tpu_name TPU_NAME   Name of TPU\n",
            "  --tpu_zone TPU_ZONE   Zone of TPU\n",
            "  --gcp_project GCP_PROJECT\n",
            "                        Name of Cloud TPU-enabled project\n",
            "  --poly_power POLY_POWER\n",
            "                        Power for the Polynomial decay LR scheduler.\n",
            "  --xla [XLA]           Whether to activate the XLA compilation or not\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQKW9EiUlYVk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "055cce52-0fb4-43f1-b2bf-2b55afee169d"
      },
      "source": [
        "!python3 transformers/examples/legacy/token-classification/run_tf_ner.py \\\n",
        "--data_dir ./processed_data/ \\\n",
        "--labels labels.txt \\\n",
        "--model_name_or_path HooshvareLab/bert-base-parsbert-uncased \\\n",
        "--output_dir eval/ \\\n",
        "--max_seq_length  256 \\\n",
        "--per_device_train_batch_size 8 \\\n",
        "--save_steps 1000 \\\n",
        "--save_total_limit 1 \\\n",
        "--eval_steps 100 \\\n",
        "--logging_strategy steps \\\n",
        "--logging_steps 100 \\\n",
        "--logging_first_step True \\\n",
        "--evaluation_strategy steps \\\n",
        "--seed 1 \\\n",
        "--num_train_epochs 5 \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--do_predict"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-20 04:41:43.287043: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "[INFO|training_args.py:578] 2021-02-20 04:41:44,547 >> PyTorch: setting up devices\n",
            "[INFO|training_args.py:516] 2021-02-20 04:41:44,585 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "[INFO|training_args_tf.py:185] 2021-02-20 04:41:44,590 >> Tensorflow: setting up strategy\n",
            "2021-02-20 04:41:44.590823: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-02-20 04:41:44.590915: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2021-02-20 04:41:44.591053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-02-20 04:41:44.591631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2021-02-20 04:41:44.591673: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-02-20 04:41:44.593389: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
            "2021-02-20 04:41:44.593461: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
            "2021-02-20 04:41:44.595055: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2021-02-20 04:41:44.595409: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2021-02-20 04:41:44.597282: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-02-20 04:41:44.598452: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-02-20 04:41:44.602309: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-02-20 04:41:44.602417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-02-20 04:41:44.602988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-02-20 04:41:44.603517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
            "2021-02-20 04:41:44.604065: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-02-20 04:41:44.604184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-02-20 04:41:44.604740: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2021-02-20 04:41:44.604786: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-02-20 04:41:44.604821: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
            "2021-02-20 04:41:44.604849: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
            "2021-02-20 04:41:44.604875: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2021-02-20 04:41:44.604899: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2021-02-20 04:41:44.604921: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-02-20 04:41:44.604944: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-02-20 04:41:44.604968: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-02-20 04:41:44.605034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-02-20 04:41:44.605610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-02-20 04:41:44.606112: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
            "2021-02-20 04:41:44.606164: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-02-20 04:41:47.798785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-02-20 04:41:47.798850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
            "2021-02-20 04:41:47.798867: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
            "2021-02-20 04:41:47.799091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-02-20 04:41:47.799780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-02-20 04:41:47.800356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-02-20 04:41:47.800854: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-02-20 04:41:47.800911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13210 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "02/20/2021 04:41:47 - INFO - __main__ -   n_replicas: 1, distributed training: False, 16-bits training: False\n",
            "02/20/2021 04:41:47 - INFO - __main__ -   Training/evaluation parameters TFTrainingArguments(output_dir='eval/', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/Feb20_04-41-44_279572b6a5c4', logging_strategy=<LoggingStrategy.STEPS: 'steps'>, logging_first_step=True, logging_steps=100, save_steps=1000, save_total_limit=1, no_cuda=False, seed=1, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='eval/', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, tpu_name=None, tpu_zone=None, gcp_project=None, poly_power=1.0, xla=False)\n",
            "[INFO|configuration_utils.py:456] 2021-02-20 04:41:47,826 >> loading configuration file https://huggingface.co/HooshvareLab/bert-base-parsbert-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/d3b7c3283a6a4ad4471f59269c9de8adadfab0b05eebf49a64e046fca56cdab2.58cfea678e7bd2c1de3bfd4a5357101526b9fbc32a994b9456047e55b0afbebe\n",
            "[INFO|configuration_utils.py:492] 2021-02-20 04:41:47,826 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"B-EVE\",\n",
            "    \"1\": \"B-LOC\",\n",
            "    \"2\": \"B-NAT\",\n",
            "    \"3\": \"B-ORG\",\n",
            "    \"4\": \"B-PER\",\n",
            "    \"5\": \"B-POG\",\n",
            "    \"6\": \"I-EVE\",\n",
            "    \"7\": \"I-LOC\",\n",
            "    \"8\": \"I-NAT\",\n",
            "    \"9\": \"I-ORG\",\n",
            "    \"10\": \"I-PER\",\n",
            "    \"11\": \"I-POG\",\n",
            "    \"12\": \"O\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B-EVE\": 0,\n",
            "    \"B-LOC\": 1,\n",
            "    \"B-NAT\": 2,\n",
            "    \"B-ORG\": 3,\n",
            "    \"B-PER\": 4,\n",
            "    \"B-POG\": 5,\n",
            "    \"I-EVE\": 6,\n",
            "    \"I-LOC\": 7,\n",
            "    \"I-NAT\": 8,\n",
            "    \"I-ORG\": 9,\n",
            "    \"I-PER\": 10,\n",
            "    \"I-POG\": 11,\n",
            "    \"O\": 12\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:456] 2021-02-20 04:41:47,842 >> loading configuration file https://huggingface.co/HooshvareLab/bert-base-parsbert-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/d3b7c3283a6a4ad4471f59269c9de8adadfab0b05eebf49a64e046fca56cdab2.58cfea678e7bd2c1de3bfd4a5357101526b9fbc32a994b9456047e55b0afbebe\n",
            "[INFO|configuration_utils.py:492] 2021-02-20 04:41:47,843 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1688] 2021-02-20 04:41:47,843 >> Model name 'HooshvareLab/bert-base-parsbert-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'HooshvareLab/bert-base-parsbert-uncased' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-20 04:41:47,934 >> loading file https://huggingface.co/HooshvareLab/bert-base-parsbert-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/b80b05f64dc19f3c880b7074ef09108d0bc244e4b6f50d6dba094da0f1c231fd.6699f2ee4745b6531f79b9781879071b6ace2d2768df83889391421fb44d4474\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-20 04:41:47,934 >> loading file https://huggingface.co/HooshvareLab/bert-base-parsbert-uncased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-20 04:41:47,934 >> loading file https://huggingface.co/HooshvareLab/bert-base-parsbert-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-20 04:41:47,934 >> loading file https://huggingface.co/HooshvareLab/bert-base-parsbert-uncased/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-20 04:41:47,934 >> loading file https://huggingface.co/HooshvareLab/bert-base-parsbert-uncased/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|modeling_tf_utils.py:1222] 2021-02-20 04:41:48,054 >> loading weights file https://huggingface.co/HooshvareLab/bert-base-parsbert-uncased/resolve/main/tf_model.h5 from cache at /root/.cache/huggingface/transformers/9cdad356c7d6956a9814a7cd822941b9a7bc7f3b450273d48f5a6626d97413a4.f8b29776d705d818b1b6d82629b12d40a9867a4c1e55e043dd81c99c2e36a246.h5\n",
            "2021-02-20 04:41:48.321477: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
            "[WARNING|modeling_tf_utils.py:1270] 2021-02-20 04:41:49,396 >> All model checkpoint layers were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_tf_utils.py:1274] 2021-02-20 04:41:49,396 >> Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at HooshvareLab/bert-base-parsbert-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "02/20/2021 04:41:49 - INFO - utils_ner -   Writing example 0 of 6417\n",
            "02/20/2021 04:41:49 - INFO - utils_ner -   *** Example ***\n",
            "02/20/2021 04:41:49 - INFO - utils_ner -   guid: train-1\n",
            "02/20/2021 04:41:49 - INFO - utils_ner -   tokens: [CLS] چهارمین قهرمانی فلو ##مین ##نس ##ه در برزیل [SEP]\n",
            "02/20/2021 04:41:49 - INFO - utils_ner -   input_ids: 2 6968 5179 13879 2231 8049 1177 2028 6674 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 04:41:49 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 04:41:49 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 04:41:49 - INFO - utils_ner -   label_ids: -100 12 12 3 -100 -100 -100 12 1 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "02/20/2021 04:41:49 - INFO - utils_ner -   *** Example ***\n",
            "02/20/2021 04:41:49 - INFO - utils_ner -   guid: train-2\n",
            "02/20/2021 04:41:49 - INFO - utils_ner -   tokens: [CLS] زندگی سلام کورا ##سا ##يو بلیز سیشل و نايورو خراسان ورزشی دو بیانضباطی و دو واکنش متفاوت از فرهاد سرخهای پایتخت در اندیشه دبل [SEP]\n",
            "02/20/2021 04:41:49 - INFO - utils_ner -   input_ids: 2 2763 3132 56694 7779 5657 43533 73166 331 51087 4985 4874 2136 34955 331 2136 4104 4215 2036 8723 52933 4840 2028 6793 53073 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 04:41:49 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 04:41:49 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 04:41:49 - INFO - utils_ner -   label_ids: -100 12 12 12 -100 -100 12 12 12 12 3 9 12 12 12 12 12 12 12 4 3 9 12 12 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "02/20/2021 04:41:49 - INFO - utils_ner -   *** Example ***\n",
            "02/20/2021 04:41:49 - INFO - utils_ner -   guid: train-3\n",
            "02/20/2021 04:41:49 - INFO - utils_ner -   tokens: [CLS] سال [UNK] را سالی که در ان بایرن همه جامهای ممکن را برد و حالا [UNK] تکرار کسب تمامی عناوین ممکن لیگ قهرمانان بوندس لیگا جام حذفی المان سوپرکاپ اروپا ( سال [UNK] قهرمان جام باشگاههای جهان ) [SEP]\n",
            "02/20/2021 04:41:49 - INFO - utils_ner -   input_ids: 2 2101 1 2049 5622 2046 2028 2050 12028 2440 29809 3085 2049 3736 331 3826 1 5439 3161 3629 9337 3085 3820 7112 27507 25372 2533 10236 4016 43671 3509 9 2101 1 6000 2533 9489 2685 10 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 04:41:49 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 04:41:49 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 04:41:49 - INFO - utils_ner -   label_ids: -100 12 12 12 12 12 12 12 3 12 12 12 12 12 12 12 12 12 12 12 12 12 0 6 0 6 0 6 6 0 6 12 12 12 0 6 6 6 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "02/20/2021 04:41:49 - INFO - utils_ner -   *** Example ***\n",
            "02/20/2021 04:41:49 - INFO - utils_ner -   guid: train-4\n",
            "02/20/2021 04:41:49 - INFO - utils_ner -   tokens: [CLS] اگه از هر قریه کشور یک ازمایش dna گرفته شود باز خاد دیدیم که نه پشتون پشتون است نه تاجیک تاجیک فخر قومی و ملیتی قصه کچ ##ری قروت خاد شد مط ##من استم تقریبا ده درصد حشمت غنی هم هزاره است مثل یک سومالیایی سیا پوست ده یوتیوب نشان میداد که ده درصد چین ##ایی است دراصل از خاکی ##م و در خاکی ##م [SEP]\n",
            "02/20/2021 04:41:49 - INFO - utils_ner -   input_ids: 2 12915 2036 2202 37139 2116 2076 4764 17023 2563 2268 2266 11138 10725 2046 2505 55145 55145 2045 2505 10444 10444 14935 13097 331 24339 8468 21727 2059 94422 11138 2087 2294 2316 9661 4557 2326 2361 25549 6648 2063 16365 2045 3452 2076 52621 10117 4531 2326 15916 2555 4861 2046 2326 2361 3443 2100 2045 75108 2036 12538 1155 331 2028 12538 1155 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 04:41:49 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 04:41:49 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 04:41:49 - INFO - utils_ner -   label_ids: -100 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 2 2 12 12 2 2 12 12 12 12 12 12 -100 12 12 12 12 -100 12 12 12 12 12 12 12 12 12 12 12 2 12 12 12 3 12 12 12 12 12 12 -100 12 12 12 12 -100 12 12 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "02/20/2021 04:41:49 - INFO - utils_ner -   *** Example ***\n",
            "02/20/2021 04:41:49 - INFO - utils_ner -   guid: train-5\n",
            "02/20/2021 04:41:49 - INFO - utils_ner -   tokens: [CLS] اختلافات سپاه تهران و فرماندهی سپاه انکار ##شدنی نیست نکته این است که برای طرفین ماجرا این یک اختلاف درون خانواده انقلاب و سپاه است حالا bbc هم مرده ##خوری میکند وگرنه رسانه ملکه و سربازان فارسیزبان ##ش با جنگ و سپاه مشکل ریشهای دارند از فتحالمبین و بیتالمقدس که همه یکدست بودند [SEP]\n",
            "02/20/2021 04:41:49 - INFO - utils_ner -   input_ids: 2 7329 4008 2402 331 7032 4008 7108 15891 2380 3926 2042 2045 2046 2073 8015 5940 2042 2076 4201 4710 3045 2858 331 4008 2045 3826 14208 2063 11627 11883 2313 11874 3080 12352 331 8370 30760 1176 2037 2941 331 4008 2524 15983 2522 2036 54398 331 24449 2046 2440 19160 2820 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 04:41:49 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 04:41:49 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 04:41:49 - INFO - utils_ner -   label_ids: -100 12 3 9 12 12 3 12 -100 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 3 12 12 3 12 12 -100 12 12 12 4 12 12 12 -100 12 12 12 3 12 12 12 12 0 12 0 12 12 12 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "02/20/2021 04:42:02 - INFO - utils_ner -   Writing example 0 of 446\n",
            "02/20/2021 04:42:02 - INFO - utils_ner -   *** Example ***\n",
            "02/20/2021 04:42:02 - INFO - utils_ner -   guid: dev-1\n",
            "02/20/2021 04:42:02 - INFO - utils_ner -   tokens: [CLS] باید از امید اروپاییها برید مخصوصا ایرانیانی که در پارلمانها هستند و طوری وانمود میکنند که رژیم ایران یک رژیم عادی ولی سختگیر است [SEP]\n",
            "02/20/2021 04:42:02 - INFO - utils_ner -   input_ids: 2 2212 2036 2878 13864 22242 8779 33874 2046 2028 66309 2373 331 4624 19439 2484 2046 3958 2119 2076 3958 5168 2752 24199 2045 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 04:42:02 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 04:42:02 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 04:42:02 - INFO - utils_ner -   label_ids: -100 12 12 12 12 12 12 2 12 12 12 12 12 12 12 12 12 12 1 12 12 12 12 12 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "02/20/2021 04:42:02 - INFO - utils_ner -   *** Example ***\n",
            "02/20/2021 04:42:02 - INFO - utils_ner -   guid: dev-2\n",
            "02/20/2021 04:42:02 - INFO - utils_ner -   tokens: [CLS] پیدیاف اقتصاد سیاسی نیکی ##تین پیدا میشه سینا [SEP]\n",
            "02/20/2021 04:42:02 - INFO - utils_ner -   input_ids: 2 64971 2529 2809 12466 9847 2880 11290 8588 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 04:42:02 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 04:42:02 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 04:42:02 - INFO - utils_ner -   label_ids: -100 12 12 12 12 -100 12 12 4 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "02/20/2021 04:42:02 - INFO - utils_ner -   *** Example ***\n",
            "02/20/2021 04:42:02 - INFO - utils_ner -   guid: dev-3\n",
            "02/20/2021 04:42:02 - INFO - utils_ner -   tokens: [CLS] ذهنم به گوشههای دورتر ان بران ##م از همان کورس ( حالا به نام کاج ) اکنون یک دختر اول نمره عمومی کانک ##ور شده است یاد ان روز شوم و شام غربت در خاطرم زنده شد شمسی ##ه نشان داد که خم میشویم اما نمیش ##کنیم [UNK] [SEP]\n",
            "02/20/2021 04:42:02 - INFO - utils_ner -   input_ids: 2 18251 2031 28171 17432 2050 14648 1155 2036 2745 20921 9 3826 2031 2410 22605 10 3113 2076 4589 2389 8558 2899 21347 2034 2110 2045 2786 2050 2182 7834 331 7428 23200 2028 27099 4797 2087 11364 1177 2555 2197 2046 8097 7333 2195 7708 19493 1 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 04:42:02 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 04:42:02 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 04:42:02 - INFO - utils_ner -   label_ids: -100 12 12 12 12 12 12 -100 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 -100 12 12 12 12 12 12 12 12 12 12 12 12 12 4 -100 12 12 12 12 12 12 12 -100 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "02/20/2021 04:42:02 - INFO - utils_ner -   *** Example ***\n",
            "02/20/2021 04:42:02 - INFO - utils_ner -   guid: dev-4\n",
            "02/20/2021 04:42:02 - INFO - utils_ner -   tokens: [CLS] بیمه بیکاری حقی ##ه ##که بهتر از من میدونین هر هفته از پیرو ##ل کسر میشه و هیچ سنخیتی با ولف ##ر که کمک هزینه مستمندان هستش نداره ولی در کل موافقم وقتی میتونی کار کنی و مالیات بدی بهتره از این کمک هزینهها دور باشی [SEP]\n",
            "02/20/2021 04:42:02 - INFO - utils_ner -   input_ids: 2 2785 5231 15516 1177 2133 3446 2036 2078 87587 2202 2759 2036 11048 1173 8662 11290 331 2608 35821 2037 23921 1156 2046 2817 3145 38238 31540 18803 2752 2028 2381 27082 3043 36803 2109 8374 331 4325 6339 30664 2036 2042 2817 7748 2394 12537 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 04:42:02 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 04:42:02 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 04:42:02 - INFO - utils_ner -   label_ids: -100 12 12 12 -100 -100 12 12 12 12 12 12 12 12 -100 12 12 12 12 12 12 12 -100 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "02/20/2021 04:42:02 - INFO - utils_ner -   *** Example ***\n",
            "02/20/2021 04:42:02 - INFO - utils_ner -   guid: dev-5\n",
            "02/20/2021 04:42:02 - INFO - utils_ner -   tokens: [CLS] ششم [UNK] ) در کوچههای سرش ##ور ول میگشتم که صدای روضهای حزن ##انگیز را شنیدم دل به دریا زدم و وارد گودی کرونا شدم به عمرم ندیده بودم که کسی چنین سوزی را برای زندگی سر داده باشد نه در سوفو ##کل نه در شکسپیر و نه در گوته [SEP]\n",
            "02/20/2021 04:42:02 - INFO - utils_ner -   input_ids: 2 4969 1 10 2028 20948 10969 2034 3891 46367 2046 4873 84885 35855 11297 2049 13999 2420 2031 2628 12411 331 2480 35190 28114 5564 2031 27042 12367 4703 2046 3256 2978 8544 2049 2073 2763 2140 2488 2302 2505 2028 97609 2262 2505 2028 27500 331 2505 2028 42754 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 04:42:02 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 04:42:02 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 04:42:02 - INFO - utils_ner -   label_ids: -100 12 12 12 12 12 12 -100 12 12 12 12 12 12 -100 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 4 -100 12 12 4 12 12 12 4 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "[INFO|trainer_tf.py:117] 2021-02-20 04:42:03,231 >> You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n",
            "[INFO|trainer_tf.py:125] 2021-02-20 04:42:03,231 >> To use comet_ml logging, run `pip/conda install comet_ml` see https://www.comet.ml/docs/python-sdk/huggingface/\n",
            "2021-02-20 04:42:03.236589: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 04:42:03.237023: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4357\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:528] 2021-02-20 04:42:03,247 >> ***** Running training *****\n",
            "[INFO|trainer_tf.py:529] 2021-02-20 04:42:03,247 >>   Num examples = 6417\n",
            "[INFO|trainer_tf.py:531] 2021-02-20 04:42:03,248 >>   Num Epochs = 5\n",
            "[INFO|trainer_tf.py:532] 2021-02-20 04:42:03,248 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer_tf.py:534] 2021-02-20 04:42:03,248 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer_tf.py:536] 2021-02-20 04:42:03,248 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer_tf.py:537] 2021-02-20 04:42:03,248 >>   Steps per epoch = 803\n",
            "[INFO|trainer_tf.py:538] 2021-02-20 04:42:03,248 >>   Total optimization steps = 4015\n",
            "2021-02-20 04:42:03.281990: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
            "2021-02-20 04:42:03.282370: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2199995000 Hz\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "02/20/2021 04:42:07 - WARNING - tensorflow -   The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "02/20/2021 04:42:08 - WARNING - tensorflow -   The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py:390: UserWarning: Using `-1` to mask the loss for the token is deprecated. Please use `-100` instead.\n",
            "  return py_builtins.overload_of(f)(*args)\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "02/20/2021 04:42:13 - WARNING - tensorflow -   The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "02/20/2021 04:42:13 - WARNING - tensorflow -   The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:42:19,874 >> {'loss': 2.886144, 'learning_rate': 4.9987546e-05, 'epoch': 0.0012453300124533001, 'step': 1}\n",
            "2021-02-20 04:43:03.620292: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 04:43:03.620461: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 04:43:03,621 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 04:43:03,621 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 04:43:03,621 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 04:43:03,622 >>   Batch size = 8\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "02/20/2021 04:43:03 - WARNING - tensorflow -   The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "02/20/2021 04:43:03 - WARNING - tensorflow -   The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:43:13,440 >> {'eval_loss': 0.16354387147086008, 'eval_precision': 0.6274671052631579, 'eval_recall': 0.7480392156862745, 'eval_f1': 0.682468694096601, 'epoch': 0.12453300124533001, 'step': 100}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:43:13,444 >> {'loss': 0.33354115, 'learning_rate': 4.875467e-05, 'epoch': 0.12453300124533001, 'step': 100}\n",
            "2021-02-20 04:43:59.676152: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 04:43:59.676351: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 04:43:59,677 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 04:43:59,677 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 04:43:59,677 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 04:43:59,677 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:44:08,541 >> {'eval_loss': 0.10666415521076747, 'eval_precision': 0.7710960232783706, 'eval_recall': 0.7794117647058824, 'eval_f1': 0.7752315943442224, 'epoch': 0.24906600249066002, 'step': 200}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:44:08,545 >> {'loss': 0.23061764, 'learning_rate': 4.7509337e-05, 'epoch': 0.24906600249066002, 'step': 200}\n",
            "2021-02-20 04:44:56.643605: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 04:44:56.643767: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 04:44:56,645 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 04:44:56,645 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 04:44:56,645 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 04:44:56,645 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:45:05,618 >> {'eval_loss': 0.10818043776920863, 'eval_precision': 0.707089552238806, 'eval_recall': 0.7431372549019608, 'eval_f1': 0.7246653919694072, 'epoch': 0.37359900373599003, 'step': 300}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:45:05,621 >> {'loss': 0.19943908, 'learning_rate': 4.626401e-05, 'epoch': 0.37359900373599003, 'step': 300}\n",
            "2021-02-20 04:45:53.284553: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 04:45:53.284717: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 04:45:53,285 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 04:45:53,286 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 04:45:53,286 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 04:45:53,286 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:46:02,301 >> {'eval_loss': 0.112674994128091, 'eval_precision': 0.7343336275375111, 'eval_recall': 0.8156862745098039, 'eval_f1': 0.7728750580585231, 'epoch': 0.49813200498132004, 'step': 400}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:46:02,305 >> {'loss': 0.17572773, 'learning_rate': 4.5018674e-05, 'epoch': 0.49813200498132004, 'step': 400}\n",
            "2021-02-20 04:46:50.328680: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 04:46:50.328848: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 04:46:50,330 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 04:46:50,330 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 04:46:50,330 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 04:46:50,330 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:46:59,329 >> {'eval_loss': 0.09782794543675014, 'eval_precision': 0.7597517730496454, 'eval_recall': 0.8401960784313726, 'eval_f1': 0.7979515828677839, 'epoch': 0.6226650062266501, 'step': 500}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:46:59,332 >> {'loss': 0.16061983, 'learning_rate': 4.377335e-05, 'epoch': 0.6226650062266501, 'step': 500}\n",
            "2021-02-20 04:47:47.170099: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 04:47:47.170261: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 04:47:47,171 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 04:47:47,171 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 04:47:47,171 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 04:47:47,171 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:47:56,142 >> {'eval_loss': 0.10016053915023804, 'eval_precision': 0.7791586998087954, 'eval_recall': 0.7990196078431373, 'eval_f1': 0.7889641819941917, 'epoch': 0.7471980074719801, 'step': 600}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:47:56,146 >> {'loss': 0.1506337, 'learning_rate': 4.252802e-05, 'epoch': 0.7471980074719801, 'step': 600}\n",
            "2021-02-20 04:48:43.927899: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 04:48:43.928080: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 04:48:43,929 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 04:48:43,929 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 04:48:43,929 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 04:48:43,929 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:48:52,901 >> {'eval_loss': 0.09258008854729789, 'eval_precision': 0.7934485896269335, 'eval_recall': 0.8549019607843137, 'eval_f1': 0.8230297310051912, 'epoch': 0.8717310087173101, 'step': 700}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:48:52,905 >> {'loss': 0.14096393, 'learning_rate': 4.128269e-05, 'epoch': 0.8717310087173101, 'step': 700}\n",
            "2021-02-20 04:49:40.740534: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 04:49:40.740695: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 04:49:40,741 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 04:49:40,741 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 04:49:40,741 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 04:49:40,741 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:49:49,721 >> {'eval_loss': 0.08820620604923793, 'eval_precision': 0.7717879604672058, 'eval_recall': 0.842156862745098, 'eval_f1': 0.8054383497421472, 'epoch': 0.9962640099626401, 'step': 800}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:49:49,725 >> {'loss': 0.13281988, 'learning_rate': 4.003736e-05, 'epoch': 0.9962640099626401, 'step': 800}\n",
            "2021-02-20 04:50:39.325542: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 04:50:39.325733: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 04:50:39,326 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 04:50:39,327 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 04:50:39,327 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 04:50:39,327 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:50:48,368 >> {'eval_loss': 0.0962066650390625, 'eval_precision': 0.791743119266055, 'eval_recall': 0.846078431372549, 'eval_f1': 0.8180094786729858, 'epoch': 1.1207970112079702, 'step': 900}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:50:48,371 >> {'loss': 0.057499804, 'learning_rate': 3.8792026e-05, 'epoch': 1.1207970112079702, 'step': 900}\n",
            "2021-02-20 04:51:36.098823: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 04:51:36.098992: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 04:51:36,100 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 04:51:36,100 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 04:51:36,100 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 04:51:36,100 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:51:45,078 >> {'eval_loss': 0.10296123368399483, 'eval_precision': 0.7833969465648855, 'eval_recall': 0.8049019607843138, 'eval_f1': 0.7940038684719536, 'epoch': 1.2453300124533002, 'step': 1000}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:51:45,081 >> {'loss': 0.0554963, 'learning_rate': 3.7546695e-05, 'epoch': 1.2453300124533002, 'step': 1000}\n",
            "2021-02-20 04:51:45.155726: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 307200000 exceeds 10% of free system memory.\n",
            "2021-02-20 04:51:45.576844: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 307200000 exceeds 10% of free system memory.\n",
            "2021-02-20 04:51:46.051923: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 307200000 exceeds 10% of free system memory.\n",
            "[INFO|trainer_tf.py:595] 2021-02-20 04:52:03,354 >> Saving checkpoint for step 1000 at eval/checkpoint/ckpt-1\n",
            "2021-02-20 04:52:52.021183: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 04:52:52.021379: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 04:52:52,022 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 04:52:52,022 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 04:52:52,022 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 04:52:52,023 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:53:01,091 >> {'eval_loss': 0.09029412269592285, 'eval_precision': 0.7965451055662188, 'eval_recall': 0.8137254901960784, 'eval_f1': 0.8050436469447139, 'epoch': 1.36986301369863, 'step': 1100}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:53:01,094 >> {'loss': 0.05642747, 'learning_rate': 3.6301364e-05, 'epoch': 1.36986301369863, 'step': 1100}\n",
            "2021-02-20 04:53:48.415901: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 04:53:48.416067: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 04:53:48,417 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 04:53:48,417 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 04:53:48,417 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 04:53:48,417 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:53:57,515 >> {'eval_loss': 0.097589339528765, 'eval_precision': 0.7778785131459656, 'eval_recall': 0.8411764705882353, 'eval_f1': 0.8082901554404145, 'epoch': 1.4943960149439601, 'step': 1200}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:53:57,518 >> {'loss': 0.053922027, 'learning_rate': 3.5056037e-05, 'epoch': 1.4943960149439601, 'step': 1200}\n",
            "2021-02-20 04:54:45.524760: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 04:54:45.524912: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 04:54:45,526 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 04:54:45,526 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 04:54:45,526 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 04:54:45,526 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:54:54,465 >> {'eval_loss': 0.10171835763113839, 'eval_precision': 0.7853839037927844, 'eval_recall': 0.8323529411764706, 'eval_f1': 0.8081865778200857, 'epoch': 1.6189290161892902, 'step': 1300}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:54:54,469 >> {'loss': 0.05211727, 'learning_rate': 3.3810706e-05, 'epoch': 1.6189290161892902, 'step': 1300}\n",
            "2021-02-20 04:55:42.267270: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 04:55:42.267445: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 04:55:42,268 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 04:55:42,268 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 04:55:42,268 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 04:55:42,268 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:55:51,324 >> {'eval_loss': 0.10192649705069405, 'eval_precision': 0.7629046369203849, 'eval_recall': 0.8549019607843137, 'eval_f1': 0.806287563569117, 'epoch': 1.74346201743462, 'step': 1400}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:55:51,327 >> {'loss': 0.050226532, 'learning_rate': 3.256538e-05, 'epoch': 1.74346201743462, 'step': 1400}\n",
            "2021-02-20 04:56:39.272126: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 04:56:39.272307: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 04:56:39,273 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 04:56:39,273 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 04:56:39,273 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 04:56:39,273 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:56:48,250 >> {'eval_loss': 0.1068880558013916, 'eval_precision': 0.7737881508078994, 'eval_recall': 0.8450980392156863, 'eval_f1': 0.8078725398313028, 'epoch': 1.86799501867995, 'step': 1500}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:56:48,253 >> {'loss': 0.048913445, 'learning_rate': 3.132005e-05, 'epoch': 1.86799501867995, 'step': 1500}\n",
            "2021-02-20 04:57:36.064718: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 04:57:36.064873: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 04:57:36,065 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 04:57:36,066 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 04:57:36,066 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 04:57:36,066 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:57:45,047 >> {'eval_loss': 0.09813320636749268, 'eval_precision': 0.8186274509803921, 'eval_recall': 0.8186274509803921, 'eval_f1': 0.8186274509803921, 'epoch': 1.9925280199252802, 'step': 1600}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:57:45,050 >> {'loss': 0.0477866, 'learning_rate': 3.0074722e-05, 'epoch': 1.9925280199252802, 'step': 1600}\n",
            "2021-02-20 04:58:34.631095: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 04:58:34.631281: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 04:58:34,632 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 04:58:34,633 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 04:58:34,633 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 04:58:34,633 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:58:43,657 >> {'eval_loss': 0.10553938150405884, 'eval_precision': 0.7791970802919708, 'eval_recall': 0.8372549019607843, 'eval_f1': 0.8071833648393194, 'epoch': 2.1170610211706102, 'step': 1700}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:58:43,661 >> {'loss': 0.03220143, 'learning_rate': 2.8829389e-05, 'epoch': 2.1170610211706102, 'step': 1700}\n",
            "2021-02-20 04:59:31.629584: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 04:59:31.629747: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 04:59:31,630 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 04:59:31,631 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 04:59:31,631 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 04:59:31,631 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:59:40,624 >> {'eval_loss': 0.10601769174848284, 'eval_precision': 0.7947019867549668, 'eval_recall': 0.8235294117647058, 'eval_f1': 0.8088589311506981, 'epoch': 2.2415940224159403, 'step': 1800}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 04:59:40,627 >> {'loss': 0.031805027, 'learning_rate': 2.7584058e-05, 'epoch': 2.2415940224159403, 'step': 1800}\n",
            "2021-02-20 05:00:28.408603: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 05:00:28.408779: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 05:00:28,410 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 05:00:28,410 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 05:00:28,410 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 05:00:28,410 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:00:37,393 >> {'eval_loss': 0.10550918749400548, 'eval_precision': 0.8071705426356589, 'eval_recall': 0.8166666666666667, 'eval_f1': 0.8118908382066276, 'epoch': 2.3661270236612704, 'step': 1900}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:00:37,396 >> {'loss': 0.032445695, 'learning_rate': 2.6338728e-05, 'epoch': 2.3661270236612704, 'step': 1900}\n",
            "2021-02-20 05:01:25.135322: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 05:01:25.135475: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 05:01:25,136 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 05:01:25,136 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 05:01:25,136 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 05:01:25,136 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:01:34,099 >> {'eval_loss': 0.10648226737976074, 'eval_precision': 0.7818181818181819, 'eval_recall': 0.8431372549019608, 'eval_f1': 0.8113207547169811, 'epoch': 2.4906600249066004, 'step': 2000}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:01:34,102 >> {'loss': 0.032363307, 'learning_rate': 2.5093399e-05, 'epoch': 2.4906600249066004, 'step': 2000}\n",
            "2021-02-20 05:01:34.169823: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 307200000 exceeds 10% of free system memory.\n",
            "2021-02-20 05:01:34.427607: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 307200000 exceeds 10% of free system memory.\n",
            "[INFO|trainer_tf.py:595] 2021-02-20 05:01:55,446 >> Saving checkpoint for step 2000 at eval/checkpoint/ckpt-2\n",
            "2021-02-20 05:02:44.010536: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 05:02:44.010706: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 05:02:44,011 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 05:02:44,012 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 05:02:44,012 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 05:02:44,012 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:02:53,069 >> {'eval_loss': 0.09990197420120239, 'eval_precision': 0.8013245033112583, 'eval_recall': 0.8303921568627451, 'eval_f1': 0.8155994222436206, 'epoch': 2.61519302615193, 'step': 2100}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:02:53,072 >> {'loss': 0.031437676, 'learning_rate': 2.3848066e-05, 'epoch': 2.61519302615193, 'step': 2100}\n",
            "2021-02-20 05:03:40.508132: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 05:03:40.508312: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 05:03:40,509 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 05:03:40,509 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 05:03:40,509 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 05:03:40,509 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:03:49,572 >> {'eval_loss': 0.10434067249298096, 'eval_precision': 0.8076923076923077, 'eval_recall': 0.8235294117647058, 'eval_f1': 0.8155339805825242, 'epoch': 2.73972602739726, 'step': 2200}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:03:49,576 >> {'loss': 0.030018289, 'learning_rate': 2.2602739e-05, 'epoch': 2.73972602739726, 'step': 2200}\n",
            "2021-02-20 05:04:37.539048: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 05:04:37.539224: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 05:04:37,540 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 05:04:37,540 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 05:04:37,540 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 05:04:37,540 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:04:46,509 >> {'eval_loss': 0.10679517473493304, 'eval_precision': 0.8009433962264151, 'eval_recall': 0.8323529411764706, 'eval_f1': 0.8163461538461538, 'epoch': 2.86425902864259, 'step': 2300}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:04:46,513 >> {'loss': 0.028730635, 'learning_rate': 2.1357411e-05, 'epoch': 2.86425902864259, 'step': 2300}\n",
            "2021-02-20 05:05:34.275079: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 05:05:34.275235: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 05:05:34,276 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 05:05:34,276 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 05:05:34,276 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 05:05:34,276 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:05:43,262 >> {'eval_loss': 0.10824518544333321, 'eval_precision': 0.8194980694980695, 'eval_recall': 0.8323529411764706, 'eval_f1': 0.8258754863813229, 'epoch': 2.9887920298879203, 'step': 2400}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:05:43,265 >> {'loss': 0.027390452, 'learning_rate': 2.0112078e-05, 'epoch': 2.9887920298879203, 'step': 2400}\n",
            "2021-02-20 05:06:32.834581: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 05:06:32.834762: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 05:06:32,836 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 05:06:32,836 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 05:06:32,836 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 05:06:32,836 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:06:41,888 >> {'eval_loss': 0.11340159177780151, 'eval_precision': 0.7939793038570084, 'eval_recall': 0.8274509803921568, 'eval_f1': 0.8103696591454633, 'epoch': 3.1133250311332503, 'step': 2500}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:06:41,891 >> {'loss': 0.014549365, 'learning_rate': 1.886675e-05, 'epoch': 3.1133250311332503, 'step': 2500}\n",
            "2021-02-20 05:07:29.866042: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 05:07:29.866196: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 05:07:29,867 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 05:07:29,867 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 05:07:29,867 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 05:07:29,867 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:07:38,895 >> {'eval_loss': 0.11422632421766009, 'eval_precision': 0.8070009460737938, 'eval_recall': 0.8362745098039216, 'eval_f1': 0.8213769860375542, 'epoch': 3.2378580323785804, 'step': 2600}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:07:38,898 >> {'loss': 0.015772622, 'learning_rate': 1.7621418e-05, 'epoch': 3.2378580323785804, 'step': 2600}\n",
            "2021-02-20 05:08:26.696474: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 05:08:26.696637: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 05:08:26,697 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 05:08:26,697 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 05:08:26,698 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 05:08:26,698 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:08:35,683 >> {'eval_loss': 0.11900017942701067, 'eval_precision': 0.7891344383057091, 'eval_recall': 0.8401960784313726, 'eval_f1': 0.8138651471984807, 'epoch': 3.3623910336239105, 'step': 2700}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:08:35,686 >> {'loss': 0.01515047, 'learning_rate': 1.6376089e-05, 'epoch': 3.3623910336239105, 'step': 2700}\n",
            "2021-02-20 05:09:23.388304: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 05:09:23.388474: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 05:09:23,389 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 05:09:23,389 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 05:09:23,389 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 05:09:23,389 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:09:32,355 >> {'eval_loss': 0.1115902236529759, 'eval_precision': 0.8099331423113658, 'eval_recall': 0.8313725490196079, 'eval_f1': 0.8205128205128206, 'epoch': 3.4869240348692405, 'step': 2800}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:09:32,358 >> {'loss': 0.014823321, 'learning_rate': 1.5130757e-05, 'epoch': 3.4869240348692405, 'step': 2800}\n",
            "2021-02-20 05:10:20.085984: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 05:10:20.086158: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 05:10:20,087 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 05:10:20,087 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 05:10:20,087 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 05:10:20,087 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:10:29,046 >> {'eval_loss': 0.11727488892418998, 'eval_precision': 0.7948003714020427, 'eval_recall': 0.8392156862745098, 'eval_f1': 0.8164043872198379, 'epoch': 3.6114570361145706, 'step': 2900}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:10:29,049 >> {'loss': 0.014049555, 'learning_rate': 1.38854275e-05, 'epoch': 3.6114570361145706, 'step': 2900}\n",
            "2021-02-20 05:11:16.800220: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 05:11:16.800390: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 05:11:16,801 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 05:11:16,801 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 05:11:16,801 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 05:11:16,801 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:11:25,791 >> {'eval_loss': 0.11646346535001482, 'eval_precision': 0.8039399624765479, 'eval_recall': 0.8401960784313726, 'eval_f1': 0.8216682646212848, 'epoch': 3.7359900373599, 'step': 3000}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:11:25,794 >> {'loss': 0.013922906, 'learning_rate': 1.26401e-05, 'epoch': 3.7359900373599, 'step': 3000}\n",
            "[INFO|trainer_tf.py:595] 2021-02-20 05:11:46,700 >> Saving checkpoint for step 3000 at eval/checkpoint/ckpt-3\n",
            "2021-02-20 05:12:35.322534: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 05:12:35.322717: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 05:12:35,324 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 05:12:35,324 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 05:12:35,324 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 05:12:35,324 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:12:44,371 >> {'eval_loss': 0.12156687464032855, 'eval_precision': 0.7906764168190128, 'eval_recall': 0.8480392156862745, 'eval_f1': 0.8183538315988648, 'epoch': 3.8605230386052303, 'step': 3100}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:12:44,374 >> {'loss': 0.013188804, 'learning_rate': 1.1394769e-05, 'epoch': 3.8605230386052303, 'step': 3100}\n",
            "2021-02-20 05:13:31.748407: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 05:13:31.748568: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 05:13:31,749 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 05:13:31,749 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 05:13:31,749 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 05:13:31,749 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:13:40,832 >> {'eval_loss': 0.11933504683630806, 'eval_precision': 0.8031865042174321, 'eval_recall': 0.8401960784313726, 'eval_f1': 0.8212745567800671, 'epoch': 3.9850560398505603, 'step': 3200}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:13:40,835 >> {'loss': 0.012804166, 'learning_rate': 1.014944e-05, 'epoch': 3.9850560398505603, 'step': 3200}\n",
            "2021-02-20 05:14:30.553324: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 05:14:30.553489: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 05:14:30,554 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 05:14:30,554 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 05:14:30,554 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 05:14:30,554 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:14:39,531 >> {'eval_loss': 0.12025408233915057, 'eval_precision': 0.8131553860819828, 'eval_recall': 0.8362745098039216, 'eval_f1': 0.8245529241179314, 'epoch': 4.109589041095891, 'step': 3300}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:14:39,534 >> {'loss': 0.010758051, 'learning_rate': 8.904108e-06, 'epoch': 4.109589041095891, 'step': 3300}\n",
            "2021-02-20 05:15:27.300679: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 05:15:27.300843: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 05:15:27,301 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 05:15:27,302 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 05:15:27,302 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 05:15:27,302 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:15:36,327 >> {'eval_loss': 0.1226750101361956, 'eval_precision': 0.8050609184629803, 'eval_recall': 0.842156862745098, 'eval_f1': 0.82319118351701, 'epoch': 4.2341220423412205, 'step': 3400}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:15:36,331 >> {'loss': 0.008725481, 'learning_rate': 7.6587785e-06, 'epoch': 4.2341220423412205, 'step': 3400}\n",
            "2021-02-20 05:16:24.290058: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 05:16:24.290234: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 05:16:24,291 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 05:16:24,291 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 05:16:24,291 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 05:16:24,291 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:16:33,291 >> {'eval_loss': 0.12208010469164167, 'eval_precision': 0.8059701492537313, 'eval_recall': 0.8470588235294118, 'eval_f1': 0.8260038240917782, 'epoch': 4.35865504358655, 'step': 3500}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:16:33,294 >> {'loss': 0.008365322, 'learning_rate': 6.41345e-06, 'epoch': 4.35865504358655, 'step': 3500}\n",
            "2021-02-20 05:17:21.018107: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 05:17:21.018311: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 05:17:21,019 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 05:17:21,019 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 05:17:21,019 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 05:17:21,019 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:17:30,002 >> {'eval_loss': 0.12318315676280431, 'eval_precision': 0.7998154981549815, 'eval_recall': 0.85, 'eval_f1': 0.8241444866920152, 'epoch': 4.483188044831881, 'step': 3600}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:17:30,005 >> {'loss': 0.007685267, 'learning_rate': 5.168119e-06, 'epoch': 4.483188044831881, 'step': 3600}\n",
            "2021-02-20 05:18:17.978855: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 05:18:17.979051: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 05:18:17,980 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 05:18:17,980 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 05:18:17,980 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 05:18:17,980 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:18:26,973 >> {'eval_loss': 0.12157881259918213, 'eval_precision': 0.8037209302325582, 'eval_recall': 0.8470588235294118, 'eval_f1': 0.8248210023866349, 'epoch': 4.60772104607721, 'step': 3700}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:18:26,977 >> {'loss': 0.0075339167, 'learning_rate': 3.9227903e-06, 'epoch': 4.60772104607721, 'step': 3700}\n",
            "2021-02-20 05:19:14.866006: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 05:19:14.866203: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 05:19:14,867 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 05:19:14,867 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 05:19:14,867 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 05:19:14,867 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:19:23,855 >> {'eval_loss': 0.12143114634922572, 'eval_precision': 0.8192197906755471, 'eval_recall': 0.8441176470588235, 'eval_f1': 0.8314823756639305, 'epoch': 4.732254047322541, 'step': 3800}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:19:23,858 >> {'loss': 0.0072430777, 'learning_rate': 2.6774585e-06, 'epoch': 4.732254047322541, 'step': 3800}\n",
            "2021-02-20 05:20:11.650166: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 05:20:11.650384: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 05:20:11,651 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 05:20:11,651 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 05:20:11,652 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 05:20:11,652 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:20:20,650 >> {'eval_loss': 0.12223626886095319, 'eval_precision': 0.8092105263157895, 'eval_recall': 0.8441176470588235, 'eval_f1': 0.8262955854126679, 'epoch': 4.85678704856787, 'step': 3900}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:20:20,653 >> {'loss': 0.0071789483, 'learning_rate': 1.4321297e-06, 'epoch': 4.85678704856787, 'step': 3900}\n",
            "2021-02-20 05:21:08.536301: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 05:21:08.536469: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 05:21:08,537 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 05:21:08,537 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 05:21:08,537 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 05:21:08,537 >>   Batch size = 8\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:21:17,505 >> {'eval_loss': 0.12370483364377703, 'eval_precision': 0.8024118738404453, 'eval_recall': 0.8480392156862745, 'eval_f1': 0.8245948522402289, 'epoch': 4.981320049813201, 'step': 4000}\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:21:17,508 >> {'loss': 0.0069698016, 'learning_rate': 1.8680095e-07, 'epoch': 4.981320049813201, 'step': 4000}\n",
            "[INFO|trainer_tf.py:595] 2021-02-20 05:21:39,494 >> Saving checkpoint for step 4000 at eval/checkpoint/ckpt-4\n",
            "[INFO|trainer_tf.py:610] 2021-02-20 05:21:46,601 >> Training took: 0:39:43.350017\n",
            "[INFO|trainer_tf.py:785] 2021-02-20 05:21:46,602 >> Saving model in eval/\n",
            "[INFO|configuration_utils.py:311] 2021-02-20 05:21:46,621 >> Configuration saved in eval/config.json\n",
            "[INFO|modeling_tf_utils.py:1032] 2021-02-20 05:21:50,707 >> Model weights saved in eval/tf_model.h5\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-20 05:21:50,708 >> tokenizer config file saved in eval/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-20 05:21:50,708 >> Special tokens file saved in eval/special_tokens_map.json\n",
            "02/20/2021 05:21:50 - INFO - __main__ -   *** Evaluate ***\n",
            "2021-02-20 05:21:50.788138: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 05:21:50.788313: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_4386\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 05:21:50,789 >> ***** Running Evaluation *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 05:21:50,789 >>   Num examples in dataset = 446\n",
            "[INFO|trainer_tf.py:309] 2021-02-20 05:21:50,789 >>   Num examples in used in evaluation = 448\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 05:21:50,789 >>   Batch size = 8\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "02/20/2021 05:21:50 - WARNING - tensorflow -   The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "02/20/2021 05:21:50 - WARNING - tensorflow -   The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "[INFO|trainer_tf.py:404] 2021-02-20 05:22:06,603 >> {'eval_loss': 0.12373244762420654, 'eval_precision': 0.8024118738404453, 'eval_recall': 0.8480392156862745, 'eval_f1': 0.8245948522402289, 'epoch': 5.0, 'step': 4015}\n",
            "02/20/2021 05:22:06 - INFO - __main__ -   ***** Eval results *****\n",
            "02/20/2021 05:22:06 - INFO - __main__ -     eval_loss = 0.12373244762420654\n",
            "02/20/2021 05:22:06 - INFO - __main__ -     eval_precision = 0.8024118738404453\n",
            "02/20/2021 05:22:06 - INFO - __main__ -     eval_recall = 0.8480392156862745\n",
            "02/20/2021 05:22:06 - INFO - __main__ -     eval_f1 = 0.8245948522402289\n",
            "02/20/2021 05:22:06 - INFO - utils_ner -   Writing example 0 of 303\n",
            "02/20/2021 05:22:06 - INFO - utils_ner -   *** Example ***\n",
            "02/20/2021 05:22:06 - INFO - utils_ner -   guid: test-1\n",
            "02/20/2021 05:22:06 - INFO - utils_ner -   tokens: [CLS] خرداد [UNK] [UNK] کیهان [UNK] میراث سناتور مصطفی مصباح ##زاده [UNK] ساله شد کیهان لندن تنها رسانه ایرانی در تبعید است که [UNK] از جنگ دوم جهانی تاکنون قدمت و یک موسسه غصب شده در تهران [UNK] دارد [SEP]\n",
            "02/20/2021 05:22:06 - INFO - utils_ner -   input_ids: 2 3604 1 1 8567 1 5302 10842 7508 13018 4067 1 3503 2087 8567 6236 2678 3080 2732 2028 14260 2045 2046 1 2036 2941 2969 2753 3573 10972 331 2076 3914 30459 2110 2028 2402 1 2192 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 05:22:06 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 05:22:06 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 05:22:06 - INFO - utils_ner -   label_ids: -100 12 12 12 3 12 12 12 4 10 -100 12 12 12 3 1 12 12 2 12 12 12 12 12 12 0 6 6 12 12 12 12 12 12 12 12 1 12 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "02/20/2021 05:22:06 - INFO - utils_ner -   *** Example ***\n",
            "02/20/2021 05:22:06 - INFO - utils_ner -   guid: test-2\n",
            "02/20/2021 05:22:06 - INFO - utils_ner -   tokens: [CLS] زین سرای بیکسی کس مج ##و محنت عاشقی ز بیکس مج ##و طرب عاشقانهای ##ی رواد ##ار کنون به کس و بیکس کنون عشق مج ##و مجتبی الوندی [SEP]\n",
            "02/20/2021 05:22:06 - INFO - utils_ner -   input_ids: 2 8100 15277 99583 2640 2171 1154 57111 24701 316 43504 2171 1154 56918 43553 1158 68136 2027 6189 2031 2640 331 43504 6189 6090 2171 1154 9082 64746 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 05:22:06 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 05:22:06 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 05:22:06 - INFO - utils_ner -   label_ids: -100 12 12 12 12 12 -100 12 12 12 12 12 -100 12 12 -100 12 -100 12 12 12 12 12 12 12 12 -100 4 10 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "02/20/2021 05:22:06 - INFO - utils_ner -   *** Example ***\n",
            "02/20/2021 05:22:06 - INFO - utils_ner -   guid: test-3\n",
            "02/20/2021 05:22:06 - INFO - utils_ner -   tokens: [CLS] تا حالا چیزی از اپولو [UNK] شنیدید سفر ناموفق ناسا به ماه که بخاطر اشکالی که توی فضاپیما پیش اومد مجبور به فرود اضطراری شد و هرگز به ماه نرسید توی اون سفر جان ( جک ) سوی ##گرت داره به مرکز ناسا در هوستون گزارش میده و این عبارت رو میگه [UNK] هوستون ما یه مشکل داریم [UNK] معنی این اصطلاح ( [UNK] ) [SEP]\n",
            "02/20/2021 05:22:06 - INFO - utils_ner -   input_ids: 2 2093 3826 3678 2036 27304 1 37707 3077 13870 12597 2031 2306 2046 10014 13260 2046 5545 26508 2226 22092 5187 2031 4085 10834 2087 331 5776 2031 2306 9732 5545 5726 3077 3059 9 12178 10 2629 90500 11723 2031 3040 12597 2028 43846 2248 2422 331 2042 4726 2071 19534 1 43846 2179 5719 2524 3194 1 5749 2042 6680 9 1 10 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 05:22:06 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 05:22:06 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 05:22:06 - INFO - utils_ner -   label_ids: -100 12 12 12 12 12 12 12 12 12 3 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 4 12 4 12 4 -100 12 12 12 3 12 1 12 12 12 12 12 12 12 12 1 12 12 12 12 12 12 12 12 12 12 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "02/20/2021 05:22:06 - INFO - utils_ner -   *** Example ***\n",
            "02/20/2021 05:22:06 - INFO - utils_ner -   guid: test-4\n",
            "02/20/2021 05:22:06 - INFO - utils_ner -   tokens: [CLS] وظیفه اصلی ارتش حفاظت از ایران هست و معمولا کمبود بودجه دارند و امکانات کم ولی وظی ##ه اصلی سپاه حفظ نظام هست به هر شرایطی و معمولا بهترین بودجه و امکانات نظامی در اختیارشان هست [SEP]\n",
            "02/20/2021 05:22:06 - INFO - utils_ner -   input_ids: 2 4549 2832 4207 5120 2036 2119 2209 331 4352 4854 3319 2522 331 4499 2230 2752 4385 1177 2832 4008 3530 2834 2209 2031 2202 4258 331 4352 3362 3319 331 4499 3624 2028 22563 2209 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 05:22:06 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 05:22:06 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 05:22:06 - INFO - utils_ner -   label_ids: -100 12 12 3 12 12 1 12 12 12 12 12 12 12 12 12 12 12 -100 12 3 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "02/20/2021 05:22:06 - INFO - utils_ner -   *** Example ***\n",
            "02/20/2021 05:22:06 - INFO - utils_ner -   guid: test-5\n",
            "02/20/2021 05:22:06 - INFO - utils_ner -   tokens: [CLS] اخه تو عکس ##ایی که منتشر شد چند تا هواپیما هم بود کو هواپیماها ##ش ایسنا بلف میای روح و روان مردم دربند ظلم رو با شوخی ##ای خبری ##ت قلقلک نده بذار به درد خودرو و سکه و دلار و بورس و دارو و مایحتاج زندگی و مسکن و بسوز ##یم و زیر فشار انقلاب بسازیم وخوش باشیم که در طریقت ما کافری ##ست رنجی ##دن [SEP]\n",
            "02/20/2021 05:22:06 - INFO - utils_ner -   input_ids: 2 26485 2092 3679 2100 2046 2979 2087 2429 2093 5340 2063 2083 2873 11797 1176 3715 42943 15899 2845 331 4139 2342 22666 9539 2071 2037 9595 2026 2814 1174 55802 6176 47144 2031 4810 2331 331 4579 331 2707 331 3112 331 3955 331 20627 2763 331 2794 331 33683 2053 331 2516 3657 2858 15759 77479 3545 2046 2028 30651 2179 90811 2030 37765 2805 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 05:22:06 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 05:22:06 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "02/20/2021 05:22:06 - INFO - utils_ner -   label_ids: -100 12 12 12 -100 12 12 12 12 12 12 12 12 12 12 -100 3 12 12 12 12 12 12 12 12 12 12 12 -100 12 -100 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 -100 12 12 12 12 12 12 12 12 12 12 12 12 -100 12 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
            "2021-02-20 05:22:07.327396: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:454] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
            "2021-02-20 05:22:07.327552: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"TensorDataset/_1\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_flat_map_fn_95284\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT32\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "[INFO|trainer_tf.py:306] 2021-02-20 05:22:07,328 >> ***** Running Prediction *****\n",
            "[INFO|trainer_tf.py:307] 2021-02-20 05:22:07,328 >>   Num examples in dataset = 303\n",
            "[INFO|trainer_tf.py:310] 2021-02-20 05:22:07,328 >>   Batch size = 8\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "02/20/2021 05:22:13 - WARNING - tensorflow -   The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "02/20/2021 05:22:13 - WARNING - tensorflow -   The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "02/20/2021 05:22:15 - INFO - __main__ -   \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         EVE       0.35      0.50      0.41        14\n",
            "         LOC       0.81      0.83      0.82       221\n",
            "         NAT       0.68      1.00      0.81        30\n",
            "         ORG       0.70      0.69      0.69       129\n",
            "         PER       0.90      0.91      0.91       244\n",
            "         POG       0.94      0.77      0.85        22\n",
            "\n",
            "   micro avg       0.80      0.83      0.82       660\n",
            "   macro avg       0.73      0.78      0.75       660\n",
            "weighted avg       0.81      0.83      0.82       660\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CpDYRhtpszm"
      },
      "source": [
        "!rm -rf runs eval"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INDb-IQoO6Lv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d1c39dc-5af8-4240-ca3d-b9d78dafb48a"
      },
      "source": [
        "!zip -r eval.zip eval"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: eval/ (stored 0%)\n",
            "  adding: eval/checkpoint/ (stored 0%)\n",
            "  adding: eval/checkpoint/ckpt-4.index (deflated 82%)\n",
            "  adding: eval/checkpoint/checkpoint (deflated 40%)\n",
            "  adding: eval/checkpoint/ckpt-4.data-00000-of-00001 (deflated 30%)\n",
            "  adding: eval/tf_model.h5 (deflated 8%)\n",
            "  adding: eval/test_predictions.txt (deflated 71%)\n",
            "  adding: eval/tokenizer_config.json (deflated 45%)\n",
            "  adding: eval/eval_results.txt (deflated 27%)\n",
            "  adding: eval/test_results.txt (deflated 64%)\n",
            "  adding: eval/special_tokens_map.json (deflated 40%)\n",
            "  adding: eval/config.json (deflated 54%)\n",
            "  adding: eval/vocab.txt (deflated 62%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsXoweocoGFr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a51b688-e31a-4853-a057-a145a0e940fb"
      },
      "source": [
        "!zip -r runs.zip runs"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: runs/ (stored 0%)\n",
            "  adding: runs/Feb20_04-41-44_279572b6a5c4/ (stored 0%)\n",
            "  adding: runs/Feb20_04-41-44_279572b6a5c4/events.out.tfevents.1613796123.279572b6a5c4.905.4401.v2 (deflated 72%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGSUbyZn5871",
        "outputId": "319a6b4c-e430-456e-fa03-c3961ecd61e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls -lh"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 1.9G\n",
            "drwxr-xr-x  2 root root 4.0K Feb 20 03:53 data\n",
            "-rw-r--r--  1 root root 161K Feb 20 03:53 dev.txt.tmp\n",
            "drwxr-xr-x  3 root root 4.0K Feb 20 05:22 eval\n",
            "-rw-r--r--  1 root root 1.9G Feb 20 05:26 eval.zip\n",
            "-rw-r--r--  1 root root   74 Feb 20 03:54 labels.txt\n",
            "-rw-r--r--  1 root root  991 Feb 20 03:53 preprocess.py\n",
            "drwxr-xr-x  2 root root 4.0K Feb 20 03:54 processed_data\n",
            "drwxr-xr-x  3 root root 4.0K Feb 20 04:42 runs\n",
            "-rw-r--r--  1 root root 7.1K Feb 20 05:22 runs.zip\n",
            "drwxr-xr-x  1 root root 4.0K Feb 16 16:35 sample_data\n",
            "-rw-r--r--  1 root root 109K Feb 20 03:53 test.txt.tmp\n",
            "-rw-r--r--  1 root root 2.3M Feb 20 03:53 train.txt.tmp\n",
            "drwxr-xr-x 15 root root 4.0K Feb 20 03:53 transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5UY69RBEO2G",
        "outputId": "0976d7ce-4d1f-4b44-948a-56ddbb8e5c6f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kX_jAaC4EROM"
      },
      "source": [
        "!mv eval.zip drive/MyDrive/"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6Yje1VxErbH"
      },
      "source": [
        "!mv runs.zip drive/MyDrive/"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5nIzaStEwow"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}